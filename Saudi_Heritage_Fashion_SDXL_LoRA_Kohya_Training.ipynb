{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVqCAgSmie4"
      },
      "source": [
        "# **I. Prepare Environment**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "if int(np.__version__[0]) > 1:\n",
        "  print(\"Warning: This trainer requires Numpy 1.26.4 but Colab forced us to use a newer version. Your session will crash as we reload it.  Please continue running from here once you come back.\")\n",
        "  import os\n",
        "  os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "GLcULNDSNEXv",
        "outputId": "4209db57-7a17-45f8-aa4c-d33bbe6b699b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2. Mount Drive"
      ],
      "metadata": {
        "id": "LaOtNAgxkHD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOGt1TTvDW6Q",
        "outputId": "b2d983ee-4842-4535-9dfb-12d3e7fca687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Saudi-Heritage-GenAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0fwAN7eDpBV",
        "outputId": "f2a9be03-4790-4743-947a-4fa8702dd652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Saudi-Heritage-GenAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uazcxZGyKels",
        "outputId": "67f33f99-1b87-4272-b8f4-a7d45d405d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Saudi-Heritage-GenAI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3q60di584x",
        "outputId": "68eb61f5-da45-4c15-b38b-55be69f31425",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "lz4 is already the newest version (1.9.3-2build2).\n",
            "aria2 is already the newest version (1.36.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "--2025-06-27 21:26:41--  https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/669786276/620e2e64-be9f-4599-904f-18ee3811e159?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250627%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250627T212641Z&X-Amz-Expires=1800&X-Amz-Signature=e6cefcfee0e018395cb20a83d02bdfaa53ab3b9bee8c902e3b2444207a3bffe8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dlibtcmalloc_minimal.so.4&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-06-27 21:26:41--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/669786276/620e2e64-be9f-4599-904f-18ee3811e159?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250627%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250627T212641Z&X-Amz-Expires=1800&X-Amz-Signature=e6cefcfee0e018395cb20a83d02bdfaa53ab3b9bee8c902e3b2444207a3bffe8&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dlibtcmalloc_minimal.so.4&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 373960 (365K) [application/octet-stream]\n",
            "Saving to: ‘/content/libtcmalloc_minimal.so.4’\n",
            "\n",
            "/content/libtcmallo 100%[===================>] 365.20K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-06-27 21:26:41 (11.4 MB/s) - ‘/content/libtcmalloc_minimal.so.4’ saved [373960/373960]\n",
            "\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires accelerate>=0.21.0, but you have accelerate 0.19.0 which is incompatible.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.15.1 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.6.0 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires accelerate>=0.21.0, but you have accelerate 0.19.0 which is incompatible.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.15.1 which is incompatible.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.6.0 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.15.1 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement xformers==0.0.19 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.16rc424, 0.0.16rc425, 0.0.16, 0.0.20, 0.0.21, 0.0.22, 0.0.22.post7, 0.0.23, 0.0.23.post1, 0.0.24, 0.0.25, 0.0.25.post1, 0.0.26.post1, 0.0.27, 0.0.27.post1, 0.0.27.post2, 0.0.28, 0.0.28.post1, 0.0.28.post2, 0.0.28.post3, 0.0.29, 0.0.29.post1, 0.0.29.post2, 0.0.29.post3, 0.0.30, 0.0.31.dev1058, 0.0.31, 0.0.32.dev1060)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for xformers==0.0.19\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: prodigyopt==1.0 in /usr/local/lib/python3.11/dist-packages (1.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Found existing installation: flax 0.8.4\n",
            "Uninstalling flax-0.8.4:\n",
            "  Successfully uninstalled flax-0.8.4\n",
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Collecting flax\n",
            "  Using cached flax-0.10.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: jax==0.4.23 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (0.4.23)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.23->jax[cuda11_pip]==0.4.23) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax==0.4.23->jax[cuda11_pip]==0.4.23) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.23->jax[cuda11_pip]==0.4.23) (1.15.3)\n",
            "Requirement already satisfied: jaxlib==0.4.23+cuda11.cudnn86 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (0.4.23+cuda11.cudnn86)\n",
            "Requirement already satisfied: nvidia-cublas-cu11>=11.11 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11>=11.8 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu11>=11.8 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11>=11.8 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11>=8.8 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cufft-cu11>=10.9 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11>=11.4 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11>=11.7 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11>=2.18.3 in /usr/local/lib/python3.11/dist-packages (from jax[cuda11_pip]==0.4.23) (2.21.5)\n",
            "INFO: pip is looking at multiple versions of flax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached flax-0.10.5-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached flax-0.10.4-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached flax-0.10.3-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached flax-0.10.2-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached flax-0.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached flax-0.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Using cached flax-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "INFO: pip is still looking at multiple versions of flax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached flax-0.8.5-py3-none-any.whl.metadata (10 kB)\n",
            "  Using cached flax-0.8.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax) (0.2.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax) (0.5.16)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.74)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax) (4.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax->flax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.11/dist-packages (from optax->flax) (0.1.86)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (1.12.2)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (3.20.3)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.86->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.23.0)\n",
            "Using cached flax-0.8.4-py3-none-any.whl (698 kB)\n",
            "Installing collected packages: flax\n",
            "Successfully installed flax-0.8.4\n"
          ]
        }
      ],
      "source": [
        "# @title ## **1.1. Install Kohya Trainer**\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import time\n",
        "import requests\n",
        "import torch\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "from google.colab import drive\n",
        "\n",
        "%store -r\n",
        "\n",
        "# root_dir\n",
        "root_dir          = \"/content\"\n",
        "drive_dir         = os.path.join(root_dir, \"drive/MyDrive\")\n",
        "deps_dir          = os.path.join(root_dir, \"deps\")\n",
        "repo_dir          = os.path.join(root_dir, \"kohya-trainer\")\n",
        "training_dir      = os.path.join(root_dir, \"LoRA\")\n",
        "pretrained_model  = os.path.join(root_dir, \"pretrained_model\")\n",
        "vae_dir           = os.path.join(root_dir, \"vae\")\n",
        "lora_dir          = os.path.join(root_dir, \"network_weight\")\n",
        "repositories_dir  = os.path.join(root_dir, \"repositories\")\n",
        "config_dir        = os.path.join(training_dir, \"config\")\n",
        "tools_dir         = os.path.join(repo_dir, \"tools\")\n",
        "finetune_dir      = os.path.join(repo_dir, \"finetune\")\n",
        "accelerate_config = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "for store in [\"root_dir\", \"repo_dir\", \"training_dir\", \"pretrained_model\", \"vae_dir\", \"repositories_dir\", \"accelerate_config\", \"tools_dir\", \"finetune_dir\", \"config_dir\"]:\n",
        "    with capture.capture_output() as cap:\n",
        "        %store {store}\n",
        "        del cap\n",
        "\n",
        "repo_dict = {\n",
        "    \"xjdeng/kohya-trainer (forked repo, stable, optimized for colab use)\" : \"https://github.com/qaneel/kohya-trainer\",\n",
        "    \"kohya-ss/sd-scripts (original repo, latest update)\"                    : \"https://github.com/kohya-ss/sd-scripts\",\n",
        "}\n",
        "\n",
        "repository        = \"xjdeng/kohya-trainer (forked repo, stable, optimized for colab use)\" #@param [\"qaneel/kohya-trainer (forked repo, stable, optimized for colab use)\", \"kohya-ss/sd-scripts (original repo, latest update)\"] {allow-input: true}\n",
        "repo_url          = repo_dict[repository]\n",
        "branch            = \"main\"  # @param {type: \"string\"}\n",
        "\n",
        "def clone_repo(url, dir, branch):\n",
        "    if not os.path.exists(dir):\n",
        "       !git clone -b {branch} {url} {dir}\n",
        "\n",
        "\n",
        "def setup_directories():\n",
        "    global output_dir\n",
        "\n",
        "    output_dir      = \"/content/drive/MyDrive/Saudi-Heritage-GenAI/saudi-heritage-lora-wb\"# @param {type: \"string\"}\n",
        "\n",
        "    for dir in [training_dir, config_dir, pretrained_model, vae_dir, repositories_dir, output_dir]:\n",
        "        os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "def pastebin_reader(id):\n",
        "    if \"pastebin.com\" in id:\n",
        "        url = id\n",
        "        if 'raw' not in url:\n",
        "                url = url.replace('pastebin.com', 'pastebin.com/raw')\n",
        "    else:\n",
        "        url = \"https://pastebin.com/raw/\" + id\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    lines = response.text.split('\\n')\n",
        "    return lines\n",
        "\n",
        "def install_repository():\n",
        "    global infinite_image_browser_dir, voldy, discordia_archivum_dir\n",
        "\n",
        "    _, voldy = pastebin_reader(\"kq6ZmHFU\")[:2]\n",
        "\n",
        "    infinite_image_browser_url  = f\"https://github.com/zanllp/{voldy}-infinite-image-browsing.git\"\n",
        "    infinite_image_browser_dir  = os.path.join(repositories_dir, f\"infinite-image-browsing\")\n",
        "    infinite_image_browser_deps = os.path.join(infinite_image_browser_dir, \"requirements.txt\")\n",
        "\n",
        "    discordia_archivum_url = \"https://github.com/Linaqruf/discordia-archivum\"\n",
        "    discordia_archivum_dir = os.path.join(repositories_dir, \"discordia-archivum\")\n",
        "    discordia_archivum_deps = os.path.join(discordia_archivum_dir, \"requirements.txt\")\n",
        "\n",
        "    clone_repo(infinite_image_browser_url, infinite_image_browser_dir, \"main\")\n",
        "    clone_repo(discordia_archivum_url, discordia_archivum_dir, \"main\")\n",
        "\n",
        "    !pip install -q --upgrade -r {infinite_image_browser_deps}  numpy==1.26.4\n",
        "    !pip install python-dotenv  numpy==1.26.4\n",
        "    !pip install -q --upgrade -r {discordia_archivum_deps}  numpy==1.26.4\n",
        "\n",
        "def install_dependencies():\n",
        "    requirements_file = os.path.join(repo_dir, \"requirements.txt\")\n",
        "    model_util        = os.path.join(repo_dir, \"library/model_util.py\")\n",
        "    gpu_info          = getoutput('nvidia-smi')\n",
        "    t4_xformers_wheel = \"https://github.com/Linaqruf/colab-xformers/releases/download/0.0.20/xformers-0.0.20+1d635e1.d20230519-cp310-cp310-linux_x86_64.whl\"\n",
        "\n",
        "    !apt install aria2 lz4\n",
        "    !wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "    !pip install -q --upgrade -r {requirements_file}\n",
        "\n",
        "    if '2.0.1+cu118' in torch.__version__:\n",
        "        if 'T4' in gpu_info:\n",
        "            !pip install -q {t4_xformers_wheel}  numpy==1.26.4\n",
        "        else:\n",
        "            !pip install -q xformers==0.0.20  numpy==1.26.4\n",
        "    else:\n",
        "        !pip install -q numpy==1.26.4 torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 torchtext==0.15.1 torchdata==0.6.0 --extra-index-url https://download.pytorch.org/whl/cu118 -U\n",
        "        !pip install -q numpy==1.26.4 xformers==0.0.19 triton==2.0.0 -U\n",
        "\n",
        "    from accelerate.utils import write_basic_config\n",
        "\n",
        "    if not os.path.exists(accelerate_config):\n",
        "        write_basic_config(save_location=accelerate_config)\n",
        "\n",
        "def prepare_environment():\n",
        "    os.environ[\"LD_PRELOAD\"] = \"/content/libtcmalloc_minimal.so.4\"\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "\n",
        "def main():\n",
        "    os.chdir(root_dir)\n",
        "    clone_repo(repo_url, repo_dir, branch)\n",
        "    os.chdir(repo_dir)\n",
        "    setup_directories()\n",
        "    install_repository()\n",
        "    install_dependencies()\n",
        "    prepare_environment()\n",
        "\n",
        "main()\n",
        "!pip install prodigyopt==1.0  numpy==1.26.4\n",
        "!pip uninstall -y flax\n",
        "!pip install flax  numpy==1.26.4 \"jax[cuda11_pip]==0.4.23\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrYGu-WxFbsq",
        "outputId": "c79efbbb-b0b2-410c-bed3-ad70fe442bbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempt to load diffusers model instead due to hardware constraints.\n",
            "Diffusers model is loaded : stabilityai/stable-diffusion-xl-base-1.0\n",
            "\n",
            "\n",
            "Selected model: stabilityai/stable-diffusion-xl-base-1.0\n"
          ]
        }
      ],
      "source": [
        "# @title ## **1.2. Download SDXL**\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "import gdown\n",
        "import requests\n",
        "import subprocess\n",
        "from IPython.utils import capture\n",
        "from urllib.parse import urlparse, unquote\n",
        "from pathlib import Path\n",
        "from huggingface_hub import HfFileSystem\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "%store -r\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "# @markdown Place your Huggingface token [here](https://huggingface.co/settings/tokens) to download gated models.\n",
        "\n",
        "HUGGINGFACE_TOKEN     = \"\" #@param {type: \"string\"}\n",
        "LOAD_DIFFUSERS_MODEL  = True #@param {type: \"boolean\"}\n",
        "SDXL_MODEL_URL        = \"stabilityai/stable-diffusion-xl-base-1.0\" # @param [\"gsdf/CounterfeitXL\", \"Linaqruf/animagine-xl\", \"stabilityai/stable-diffusion-xl-base-1.0\",\"xjdeng/pony-xl-v6-diffusers\", \"PASTE MODEL URL OR GDRIVE PATH HERE\"] {allow-input: true}\n",
        "SDXL_VAE_URL          = \"Original VAE\" # @param [\"None\", \"Original VAE\", \"FP16 VAE\",\"Pony VAE\" , \"PASTE VAE URL OR GDRIVE PATH HERE\"] {allow-input: true}\n",
        "\n",
        "MODEL_URLS = {\n",
        "    \"gsdf/CounterfeitXL\"        : \"https://huggingface.co/gsdf/CounterfeitXL/resolve/main/CounterfeitXL_%CE%B2.safetensors\",\n",
        "    \"Linaqruf/animagine-xl\"   : \"https://huggingface.co/Linaqruf/animagine-xl/resolve/main/animagine-xl.safetensors\",\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\" : \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\",\n",
        "    \"xjdeng/pony-xl-v6-diffusers\": \"https://huggingface.co/xjdeng/pony-xl-v6-diffusers/resolve/main/ponyDiffusionV6XL_v6StartWithThisOne.safetensors\",\n",
        "}\n",
        "\n",
        "VAE_URLS = {\n",
        "    \"None\"                    : \"\",\n",
        "    \"Original VAE\"           : \"xjdeng/sdxl-vae\",\n",
        "    \"FP16 VAE\"           : \"madebyollin/sdxl-vae-fp16-fix\",\n",
        "    \"Pony VAE\"           : \"xjdeng/PonyVAE\",\n",
        "}\n",
        "\n",
        "\n",
        "SDXL_MODEL_URL = MODEL_URLS.get(SDXL_MODEL_URL, SDXL_MODEL_URL)\n",
        "SDXL_VAE_URL = VAE_URLS.get(SDXL_VAE_URL, SDXL_VAE_URL)\n",
        "\n",
        "def get_filename(url):\n",
        "    if any(url.endswith(ext) for ext in [\".ckpt\", \".safetensors\", \".pt\", \".pth\"]):\n",
        "        return os.path.basename(url)\n",
        "\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    if 'content-disposition' in response.headers:\n",
        "        filename = re.findall('filename=\"?([^\"]+)\"?', response.headers['content-disposition'])[0]\n",
        "    else:\n",
        "        filename = unquote(os.path.basename(urlparse(url).path))\n",
        "\n",
        "    return filename\n",
        "\n",
        "def aria2_download(dir, filename, url):\n",
        "    user_header = f\"Authorization: Bearer {HUGGINGFACE_TOKEN}\"\n",
        "    aria2_args = [\n",
        "        \"aria2c\",\n",
        "        \"--console-log-level=error\",\n",
        "        \"--summary-interval=10\",\n",
        "        f\"--header={user_header}\" if \"huggingface.co\" in url else \"\",\n",
        "        \"--continue=true\",\n",
        "        \"--max-connection-per-server=16\",\n",
        "        \"--min-split-size=1M\",\n",
        "        \"--split=16\",\n",
        "        f\"--dir={dir}\",\n",
        "        f\"--out={filename}\",\n",
        "        url\n",
        "    ]\n",
        "    subprocess.run(aria2_args)\n",
        "\n",
        "def download(url, dst):\n",
        "    print(f\"Starting downloading from {url}\")\n",
        "    filename = get_filename(url)\n",
        "    filepath = os.path.join(dst, filename)\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        gdown.download(url, filepath, quiet=False)\n",
        "    else:\n",
        "        if \"huggingface.co\" in url and \"/blob/\" in url:\n",
        "            url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        aria2_download(dst, filename, url)\n",
        "\n",
        "    print(f\"Download finished: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "def all_folders_present(base_model_url, sub_folders):\n",
        "    fs = HfFileSystem()\n",
        "    existing_folders = set(fs.ls(base_model_url, detail=False))\n",
        "\n",
        "    for folder in sub_folders:\n",
        "        full_folder_path = f\"{base_model_url}/{folder}\"\n",
        "        if full_folder_path not in existing_folders:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_total_ram_gb():\n",
        "    with open('/proc/meminfo', 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            if \"MemTotal\" in line:\n",
        "                return int(line.split()[1]) / (1024**2)  # Convert to GB\n",
        "\n",
        "def get_gpu_name():\n",
        "    try:\n",
        "        return subprocess.check_output(\"nvidia-smi --query-gpu=name --format=csv,noheader,nounits\", shell=True).decode('ascii').strip()\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    global model_path, vae_path\n",
        "    global LOAD_DIFFUSERS_MODEL  # Add this line to declare LOAD_DIFFUSERS_MODEL as global\n",
        "\n",
        "    model_path, vae_path = None, None\n",
        "\n",
        "    required_sub_folders = [\n",
        "        'scheduler',\n",
        "        'text_encoder',\n",
        "        'text_encoder_2',\n",
        "        'tokenizer',\n",
        "        'tokenizer_2',\n",
        "        'unet',\n",
        "        'vae',\n",
        "    ]\n",
        "\n",
        "    download_targets = {\n",
        "        \"model\": (SDXL_MODEL_URL, pretrained_model),\n",
        "        \"vae\": (SDXL_VAE_URL, vae_dir),\n",
        "    }\n",
        "\n",
        "    total_ram = get_total_ram_gb()\n",
        "    gpu_name = get_gpu_name()\n",
        "\n",
        "    # Check hardware constraints\n",
        "    if total_ram < 13 and gpu_name in [\"Tesla T4\", \"Tesla V100\"]:\n",
        "        print(\"Attempt to load diffusers model instead due to hardware constraints.\")\n",
        "        LOAD_DIFFUSERS_MODEL = True\n",
        "\n",
        "    for target, (url, dst) in download_targets.items():\n",
        "        if url and not url.startswith(f\"PASTE {target.upper()} URL OR GDRIVE PATH HERE\"):\n",
        "            if target == \"model\" and LOAD_DIFFUSERS_MODEL:\n",
        "                # Code for checking and handling diffusers model\n",
        "                if 'huggingface.co' in url:\n",
        "                    match = re.search(r'huggingface\\.co/([^/]+)/([^/]+)', SDXL_MODEL_URL)\n",
        "                    if match:\n",
        "                        username = match.group(1)\n",
        "                        model_name = match.group(2)\n",
        "                        url = f\"{username}/{model_name}\"\n",
        "                if all_folders_present(url, required_sub_folders):\n",
        "                    print(f\"Diffusers model is loaded : {url}\")\n",
        "                    model_path = url\n",
        "                else:\n",
        "                    print(\"Repository doesn't exist or no diffusers model detected.\")\n",
        "                    filepath = download(url, dst)  # Continue with the regular download\n",
        "                    model_path = filepath\n",
        "            else:\n",
        "                pass\n",
        "                \"\"\"\n",
        "                filepath = download(url, dst)\n",
        "\n",
        "                if target == \"model\":\n",
        "                    model_path = filepath\n",
        "                elif target == \"vae\":\n",
        "                    vae_path = filepath\n",
        "                \"\"\"\n",
        "\n",
        "            print()\n",
        "\n",
        "    if model_path:\n",
        "        print(f\"Selected model: {model_path}\")\n",
        "\n",
        "    if vae_path:\n",
        "        print(f\"Selected VAE: {vae_path}\")\n",
        "\n",
        "main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kh7CeDqK4l3Y",
        "outputId": "8009c109-c698-4a08-fed0-7f28be2c9361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stored 'train_data_dir' (str)\n",
            "Your train data directory : /content/drive/MyDrive/Saudi-Heritage-GenAI/saudi-heritage-dataset\n"
          ]
        }
      ],
      "source": [
        "# @title ## **1.3. Directory Config**\n",
        "# @markdown Specify the location of your training data in the following cell. A folder with the same name as your input will be created.\n",
        "import os\n",
        "\n",
        "%store -r\n",
        "\n",
        "train_data_dir = \"/content/drive/MyDrive/Saudi-Heritage-GenAI/saudi-heritage-dataset\"  # @param {'type' : 'string'}\n",
        "%store train_data_dir\n",
        "\n",
        "os.makedirs(train_data_dir, exist_ok=True)\n",
        "print(f\"Your train data directory : {train_data_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-0qKyEgTchp"
      },
      "source": [
        "# **III. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhgatqF3leHJ",
        "outputId": "ba387966-9f98-488d-efc1-0f6cf2c1b88e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751059763.374508    6653 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751059763.388279    6653 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Found 384 images.\n",
            "Creating a new metadata file\n",
            "Merging tags and captions into metadata json.\n",
            "100% 384/384 [00:18<00:00, 21.25it/s] \n",
            "No captions found for any of the 384 images\n",
            "All 384 images have tags\n",
            "Writing metadata: /content/LoRA/meta_clean.json\n",
            "Done!\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751059795.717288    7857 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751059795.739763    7857 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "found 384 images.\n",
            "loading existing metadata: /content/LoRA/meta_clean.json\n",
            "load VAE: xjdeng/sdxl-vae\n",
            "Downloading config.json: 100% 607/607 [00:00<00:00, 3.85MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 335M/335M [00:04<00:00, 78.5MB/s]\n",
            "100% 384/384 [10:18<00:00,  1.61s/it]\n",
            "bucket 0 (256, 1024): 2\n",
            "bucket 1 (320, 1024): 12\n",
            "bucket 2 (384, 1024): 11\n",
            "bucket 3 (448, 1024): 8\n",
            "bucket 4 (512, 1024): 9\n",
            "bucket 5 (576, 1024): 17\n",
            "bucket 6 (640, 1024): 14\n",
            "bucket 7 (704, 1024): 33\n",
            "bucket 8 (768, 1024): 31\n",
            "bucket 9 (832, 1024): 144\n",
            "bucket 10 (896, 1024): 18\n",
            "bucket 11 (960, 1024): 9\n",
            "bucket 12 (1024, 640): 1\n",
            "bucket 13 (1024, 704): 8\n",
            "bucket 14 (1024, 768): 4\n",
            "bucket 15 (1024, 832): 4\n",
            "bucket 16 (1024, 896): 4\n",
            "bucket 17 (1024, 960): 11\n",
            "bucket 18 (1024, 1024): 44\n",
            "mean ar error: 0.012313423339785287\n",
            "writing metadata: /content/LoRA/meta_lat.json\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "# @title ## **3.4. Bucketing and Latents Caching**\n",
        "%store -r\n",
        "\n",
        "# @markdown This code will create buckets based on the `bucket_resolution` provided for multi-aspect ratio training, and then convert all images within the `train_data_dir` to latents.\n",
        "bucketing_json    = os.path.join(training_dir, \"meta_lat.json\")\n",
        "metadata_json     = os.path.join(training_dir, \"meta_clean.json\")\n",
        "bucket_resolution = 1024  # @param {type:\"slider\", min:512, max:1024, step:128}\n",
        "mixed_precision   = \"no\"  # @param [\"no\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "skip_existing     = False  # @param{type:\"boolean\"}\n",
        "flip_aug          = True  # @param{type:\"boolean\"}\n",
        "# @markdown Use `clean_caption` option to clean such as duplicate tags, `women` to `girl`, etc\n",
        "clean_caption     = False #@param {type:\"boolean\"}\n",
        "#@markdown Use the `recursive` option to process subfolders as well\n",
        "recursive         = True #@param {type:\"boolean\"}\n",
        "\n",
        "metadata_config = {\n",
        "    \"_train_data_dir\": train_data_dir,\n",
        "    \"_out_json\": metadata_json,\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"clean_caption\": clean_caption\n",
        "}\n",
        "\n",
        "bucketing_config = {\n",
        "    \"_train_data_dir\": train_data_dir,\n",
        "    \"_in_json\": metadata_json,\n",
        "    \"_out_json\": bucketing_json,\n",
        "    #\"_model_name_or_path\": vae_path if vae_path else model_path,\n",
        "    \"_model_name_or_path\": SDXL_VAE_URL.replace(\"https://huggingface.co/\",\"\"),\n",
        "    \"recursive\": recursive,\n",
        "    \"full_path\": recursive,\n",
        "    \"flip_aug\": flip_aug,\n",
        "    \"skip_existing\": skip_existing,\n",
        "    \"batch_size\": 4,\n",
        "    \"max_data_loader_n_workers\": 2,\n",
        "    \"max_resolution\": f\"{bucket_resolution}, {bucket_resolution}\",\n",
        "    \"mixed_precision\": mixed_precision,\n",
        "}\n",
        "\n",
        "def generate_args(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "    return args.strip()\n",
        "\n",
        "merge_metadata_args = generate_args(metadata_config)\n",
        "prepare_buckets_args = generate_args(bucketing_config)\n",
        "\n",
        "merge_metadata_command = f\"python merge_all_to_metadata.py {merge_metadata_args}\"\n",
        "prepare_buckets_command = f\"python prepare_buckets_latents.py {prepare_buckets_args}\"\n",
        "\n",
        "os.chdir(finetune_dir)\n",
        "!{merge_metadata_command}\n",
        "time.sleep(1)\n",
        "!{prepare_buckets_command}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAZVkLuaRJ9e"
      },
      "source": [
        "# **IV. Training**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cJgLfRtlHSjw",
        "outputId": "24fabc56-00a0-460a-f34f-8feefe2ac3c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "network_module = \"networks.lora\"\n",
            "network_dim = 32\n",
            "network_alpha = 16\n",
            "network_args = []\n",
            "network_train_unet_only = false\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import toml\n",
        "\n",
        "# @title ## **4.1. LoRa: Low-Rank Adaptation Config**\n",
        "# @markdown Kohya's `LoRA` renamed to `LoRA-LierLa` and Kohya's `LoCon` renamed to `LoRA-C3Lier`, read [official announcement](https://github.com/kohya-ss/sd-scripts/blob/849bc24d205a35fbe1b2a4063edd7172533c1c01/README.md#naming-of-lora).\n",
        "network_category = \"LoRA_LierLa\"  # @param [\"LoRA_LierLa\", \"LoRA_C3Lier\", \"DyLoRA_LierLa\", \"DyLoRA_C3Lier\", \"LoCon\", \"LoHa\", \"IA3\", \"LoKR\", \"DyLoRA_Lycoris\"]\n",
        "\n",
        "# @markdown | network_category | network_dim | network_alpha | conv_dim | conv_alpha | unit |\n",
        "# @markdown | :---: | :---: | :---: | :---: | :---: | :---: |\n",
        "# @markdown | LoRA-LierLa | 32 | 1 | - | - | - |\n",
        "# @markdown | LoCon/LoRA-C3Lier | 16 | 8 | 8 | 1 | - |\n",
        "# @markdown | LoHa | 8 | 4 | 4 | 1 | - |\n",
        "# @markdown | Other Category | ? | ? | ? | ? | - |\n",
        "\n",
        "# @markdown Specify `network_args` to add `optional` training args, like for specifying each 25 block weight, read [this](https://github.com/kohya-ss/sd-scripts/blob/main/train_network_README-ja.md#%E9%9A%8E%E5%B1%A4%E5%88%A5%E5%AD%A6%E7%BF%92%E7%8E%87)\n",
        "network_args    = \"\"  # @param {'type':'string'}\n",
        "\n",
        "# @markdown ### **Linear Layer Config**\n",
        "# @markdown Used by all `network_category`. When in doubt, set `network_dim = network_alpha`\n",
        "network_dim     = 32  # @param {'type':'number'}\n",
        "network_alpha   = 16  # @param {'type':'number'}\n",
        "\n",
        "# @markdown ### **Convolutional Layer Config**\n",
        "# @markdown Only required if `network_category` is not `LoRA_LierLa`, as it involves training convolutional layers in addition to linear layers.\n",
        "conv_dim        = None  # @param {'type':'number'}\n",
        "conv_alpha      = None  # @param {'type':'number'}\n",
        "\n",
        "# @markdown ### **DyLoRA Config**\n",
        "# @markdown Only required if `network_category` is `DyLoRA_LierLa` and `DyLoRA_C3Lier`\n",
        "unit = None  # @param {'type':'number'}\n",
        "\n",
        "if isinstance(network_args, str):\n",
        "    network_args = network_args.strip()\n",
        "    if network_args.startswith('[') and network_args.endswith(']'):\n",
        "        try:\n",
        "            network_args = ast.literal_eval(network_args)\n",
        "        except (SyntaxError, ValueError) as e:\n",
        "            print(f\"Error parsing network_args: {e}\\n\")\n",
        "            network_args = []\n",
        "    elif len(network_args) > 0:\n",
        "        print(f\"WARNING! '{network_args}' is not a valid list! Put args like this: [\\\"args=1\\\", \\\"args=2\\\"]\\n\")\n",
        "        network_args = []\n",
        "    else:\n",
        "        network_args = []\n",
        "else:\n",
        "    network_args = []\n",
        "\n",
        "network_config = {\n",
        "    \"LoRA_LierLa\": {\n",
        "        \"module\": \"networks.lora\",\n",
        "        \"args\"  : []\n",
        "    },\n",
        "    \"LoRA_C3Lier\": {\n",
        "        \"module\": \"networks.lora\",\n",
        "        \"args\"  : [\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"DyLoRA_LierLa\": {\n",
        "        \"module\": \"networks.dylora\",\n",
        "        \"args\"  : [\n",
        "            f\"unit={unit}\"\n",
        "        ]\n",
        "    },\n",
        "    \"DyLoRA_C3Lier\": {\n",
        "        \"module\": \"networks.dylora\",\n",
        "        \"args\"  : [\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\",\n",
        "            f\"unit={unit}\"\n",
        "        ]\n",
        "    },\n",
        "    \"LoCon\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=locon\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"LoHa\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=loha\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"IA3\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=ia3\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"LoKR\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=lokr\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    },\n",
        "    \"DyLoRA_Lycoris\": {\n",
        "        \"module\": \"lycoris.kohya\",\n",
        "        \"args\"  : [\n",
        "            f\"algo=dylora\",\n",
        "            f\"conv_dim={conv_dim}\",\n",
        "            f\"conv_alpha={conv_alpha}\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "network_module = network_config[network_category][\"module\"]\n",
        "network_args.extend(network_config[network_category][\"args\"])\n",
        "\n",
        "lora_config = {\n",
        "    \"additional_network_arguments\": {\n",
        "        \"no_metadata\"                     : False,\n",
        "        \"network_module\"                  : network_module,\n",
        "        \"network_dim\"                     : network_dim,\n",
        "        \"network_alpha\"                   : network_alpha,\n",
        "        \"network_args\"                    : network_args,\n",
        "        \"network_train_unet_only\"         : False,\n",
        "        \"training_comment\"                : None,\n",
        "    },\n",
        "}\n",
        "\n",
        "print(toml.dumps(lora_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JNlw3u8arwir",
        "outputId": "a21a833e-fe9d-4e54-e2ad-f8a3103eb19f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[optimizer_arguments]\n",
            "optimizer_type = \"Prodigy\"\n",
            "learning_rate = 0.0001\n",
            "max_grad_norm = 0\n",
            "optimizer_args = [ \"decouple=True\", \"weight_decay=0.01\", \"betas=0.9,0.999\", \"d_coef=2\", \"use_bias_correction=True\",]\n",
            "lr_scheduler = \"constant_with_warmup\"\n",
            "lr_warmup_steps = 0.1\n",
            "text_encoder_lr = 0.0001\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import toml\n",
        "import ast\n",
        "\n",
        "# @title ## **4.2. Optimizer Config**\n",
        "# @markdown Use `Adafactor` optimizer. `RMSprop 8bit` or `Adagrad 8bit` may work. `AdamW 8bit` doesn't seem to work.\n",
        "optimizer_type = \"Prodigy\"  # @param [\"AdamW\", \"AdamW8bit\", \"Lion8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation(DAdaptAdamPreprint)\", \"DAdaptAdaGrad\", \"DAdaptAdam\", \"DAdaptAdan\", \"DAdaptAdanIP\", \"DAdaptLion\", \"DAdaptSGD\", \"AdaFactor\", \"Prodigy\"]\n",
        "# @markdown Specify `optimizer_args` to add `additional` args for optimizer, e.g: `[\"weight_decay=0.6\"]`\n",
        "optimizer_args = \"[\\\"decouple=True\\\",\\\"weight_decay=0.01\\\", \\\"betas=0.9,0.999\\\",\\\"d_coef=2\\\",\\\"use_bias_correction=True\\\"]\"  # @param {'type':'string'}\n",
        "# @markdown ### **Learning Rate Config**\n",
        "# @markdown Different `optimizer_type` and `network_category` for some condition requires different learning rate. It's recommended to set `text_encoder_lr = 1/2 * unet_lr`\n",
        "learning_rate = 1e-4  # @param {'type':'number'}\n",
        "# @markdown ### **LR Scheduler Config**\n",
        "# @markdown `lr_scheduler` provides several methods to adjust the learning rate based on the number of epochs.\n",
        "lr_scheduler = \"constant_with_warmup\"  # @param [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\", \"adafactor\"] {allow-input: false}\n",
        "lr_warmup_steps = 0.1  # @param {'type':'number'}\n",
        "# @markdown Specify `lr_scheduler_num` with `num_cycles` value for `cosine_with_restarts` or `power` value for `polynomial`\n",
        "lr_scheduler_num = 0  # @param {'type':'number'}\n",
        "\n",
        "if isinstance(optimizer_args, str):\n",
        "    optimizer_args = optimizer_args.strip()\n",
        "    if optimizer_args.startswith('[') and optimizer_args.endswith(']'):\n",
        "        try:\n",
        "            optimizer_args = ast.literal_eval(optimizer_args)\n",
        "        except (SyntaxError, ValueError) as e:\n",
        "            print(f\"Error parsing optimizer_args: {e}\\n\")\n",
        "            optimizer_args = []\n",
        "    elif len(optimizer_args) > 0:\n",
        "        print(f\"WARNING! '{optimizer_args}' is not a valid list! Put args like this: [\\\"args=1\\\", \\\"args=2\\\"]\\n\")\n",
        "        optimizer_args = []\n",
        "    else:\n",
        "        optimizer_args = []\n",
        "else:\n",
        "    optimizer_args = []\n",
        "\n",
        "optimizer_config = {\n",
        "    \"optimizer_arguments\": {\n",
        "        \"optimizer_type\"          : optimizer_type,\n",
        "        \"learning_rate\"           : learning_rate,\n",
        "        \"max_grad_norm\"           : 0,\n",
        "        \"optimizer_args\"          : optimizer_args,\n",
        "        \"lr_scheduler\"            : lr_scheduler,\n",
        "        \"lr_warmup_steps\"         : lr_warmup_steps,\n",
        "        \"lr_scheduler_num_cycles\" : lr_scheduler_num if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\"      : lr_scheduler_num if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_scheduler_type\"       : None,\n",
        "        \"lr_scheduler_args\"       : None,\n",
        "        \"text_encoder_lr\"         : learning_rate\n",
        "    },\n",
        "}\n",
        "\n",
        "print(toml.dumps(optimizer_config))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "krvamf9L-Lvc",
        "outputId": "5d38b9c4-f07a-4418-9ab3-297ef2a90970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[advanced_training_config]\n",
            "resume = \"\"\n",
            "save_state = false\n",
            "save_last_n_epochs_state = false\n",
            "caption_dropout_rate = 0\n",
            "caption_tag_dropout_rate = 0\n",
            "caption_dropout_every_n_epochs = 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title ## **4.3. Advanced Training Config** (Optional)\n",
        "import toml\n",
        "\n",
        "\n",
        "# @markdown ### **Optimizer State Config**\n",
        "save_optimizer_state      = False #@param {type:\"boolean\"}\n",
        "load_optimizer_state      = \"\" #@param {type:\"string\"}\n",
        "# @markdown ### **Noise Control**\n",
        "noise_control_type        = \"none\" #@param [\"none\", \"noise_offset\", \"multires_noise\"]\n",
        "# @markdown #### **a. Noise Offset**\n",
        "# @markdown Control and easily generating darker or light images by offset the noise when fine-tuning the model. Recommended value: `0.1`. Read [Diffusion With Offset Noise](https://www.crosslabs.org//blog/diffusion-with-offset-noise)\n",
        "noise_offset_num          = 0.0357  # @param {type:\"number\"}\n",
        "# @markdown **[Experimental]**\n",
        "# @markdown Automatically adjusts the noise offset based on the absolute mean values of each channel in the latents when used with `--noise_offset`. Specify a value around 1/10 to the same magnitude as the `--noise_offset` for best results. Set `0` to disable.\n",
        "adaptive_noise_scale      = 0.00357 # @param {type:\"number\"}\n",
        "# @markdown #### **b. Multires Noise**\n",
        "# @markdown enable multires noise with this number of iterations (if enabled, around 6-10 is recommended)\n",
        "multires_noise_iterations = 6 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "multires_noise_discount = 0.3 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "# @markdown ### **Caption Dropout**\n",
        "caption_dropout_rate = 0  # @param {type:\"number\"}\n",
        "caption_tag_dropout_rate = 0  # @param {type:\"number\"}\n",
        "caption_dropout_every_n_epochs = 0  # @param {type:\"number\"}\n",
        "# @markdown ### **Custom Train Function**\n",
        "# @markdown Gamma for reducing the weight of high-loss timesteps. Lower numbers have a stronger effect. The paper recommends `5`. Read the paper [here](https://arxiv.org/abs/2303.09556).\n",
        "min_snr_gamma             = -1 #@param {type:\"number\"}\n",
        "\n",
        "advanced_training_config = {\n",
        "    \"advanced_training_config\": {\n",
        "        \"resume\"                        : load_optimizer_state,\n",
        "        \"save_state\"                    : save_optimizer_state,\n",
        "        \"save_last_n_epochs_state\"      : save_optimizer_state,\n",
        "        \"noise_offset\"                  : noise_offset_num if noise_control_type == \"noise_offset\" else None,\n",
        "        \"adaptive_noise_scale\"          : adaptive_noise_scale if adaptive_noise_scale and noise_control_type == \"noise_offset\" else None,\n",
        "        \"multires_noise_iterations\"     : multires_noise_iterations if noise_control_type ==\"multires_noise\" else None,\n",
        "        \"multires_noise_discount\"       : multires_noise_discount if noise_control_type ==\"multires_noise\" else None,\n",
        "        \"caption_dropout_rate\"          : 0,\n",
        "        \"caption_tag_dropout_rate\"      : caption_tag_dropout_rate,\n",
        "        \"caption_dropout_every_n_epochs\": 0,\n",
        "        \"min_snr_gamma\"                 : min_snr_gamma if not min_snr_gamma == -1 else None,\n",
        "    }\n",
        "}\n",
        "\n",
        "print(toml.dumps(advanced_training_config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z4w3lfFKLjr",
        "cellView": "form",
        "outputId": "82e4db65-34e7-4c82-822d-8d7568991420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sdxl_arguments]\n",
            "cache_text_encoder_outputs = false\n",
            "no_half_vae = true\n",
            "min_timestep = 0\n",
            "max_timestep = 1000\n",
            "shuffle_caption = true\n",
            "lowram = true\n",
            "\n",
            "[model_arguments]\n",
            "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
            "\n",
            "[dataset_arguments]\n",
            "debug_dataset = false\n",
            "in_json = \"/content/LoRA/meta_lat.json\"\n",
            "train_data_dir = \"/content/drive/MyDrive/Saudi-Heritage-GenAI/saudi-heritage-dataset\"\n",
            "dataset_repeats = 1\n",
            "keep_tokens = 0\n",
            "resolution = \"768,768\"\n",
            "color_aug = false\n",
            "token_warmup_min = 1\n",
            "token_warmup_step = 0\n",
            "\n",
            "[training_arguments]\n",
            "output_dir = \"/content/drive/MyDrive/Saudi-Heritage-GenAI/saudi-heritage-lora-wb\"\n",
            "output_name = \"Saudi-Heritage-Lora \"\n",
            "save_precision = \"fp16\"\n",
            "save_every_n_epochs = 10\n",
            "train_batch_size = 1\n",
            "max_token_length = 225\n",
            "mem_eff_attn = false\n",
            "sdpa = true\n",
            "xformers = false\n",
            "max_train_epochs = 10\n",
            "max_data_loader_n_workers = 8\n",
            "persistent_data_loader_workers = true\n",
            "gradient_checkpointing = true\n",
            "gradient_accumulation_steps = 2\n",
            "mixed_precision = \"fp16\"\n",
            "\n",
            "[logging_arguments]\n",
            "log_with = \"wandb\"\n",
            "log_tracker_name = \"Saudi-Heritage-Lora \"\n",
            "logging_dir = \"/content/LoRA/logs\"\n",
            "\n",
            "[sample_prompt_arguments]\n",
            "sample_every_n_epochs = 10\n",
            "sample_sampler = \"euler_a\"\n",
            "\n",
            "[saving_arguments]\n",
            "save_model_as = \"safetensors\"\n",
            "\n",
            "[optimizer_arguments]\n",
            "optimizer_type = \"Prodigy\"\n",
            "learning_rate = 0.0001\n",
            "max_grad_norm = 0\n",
            "optimizer_args = [ \"decouple=True\", \"weight_decay=0.01\", \"betas=0.9,0.999\", \"d_coef=2\", \"use_bias_correction=True\",]\n",
            "lr_scheduler = \"constant_with_warmup\"\n",
            "lr_warmup_steps = 0.1\n",
            "text_encoder_lr = 0.0001\n",
            "\n",
            "[additional_network_arguments]\n",
            "no_metadata = false\n",
            "network_module = \"networks.lora\"\n",
            "network_dim = 32\n",
            "network_alpha = 16\n",
            "network_args = []\n",
            "network_train_unet_only = false\n",
            "\n",
            "[advanced_training_config]\n",
            "save_state = false\n",
            "save_last_n_epochs_state = false\n",
            "caption_dropout_rate = 0\n",
            "caption_tag_dropout_rate = 0\n",
            "caption_dropout_every_n_epochs = 0\n",
            "\n",
            "[prompt]\n",
            "negative_prompt = \"3d render, smooth, plastic, blurry, grainy, low-resolution, deep-fried, oversaturated\"\n",
            "width = 768\n",
            "height = 768\n",
            "scale = 12\n",
            "sample_steps = 28\n",
            "[[prompt.subset]]\n",
            "prompt = \"woman wearing najdi dress, ornate head accessory, traditional saudi fashion, heritage style\"\n",
            "\n",
            "[[prompt.subset]]\n",
            "prompt = \"najdi man, traditional embroidered thobe, shemagh headscarf, saudi central region\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title ## **4.4. Training Config**\n",
        "import toml\n",
        "import os\n",
        "from subprocess import getoutput\n",
        "import random\n",
        "\n",
        "%store -r\n",
        "\n",
        "# @markdown ### **Project Config**\n",
        "project_name                = \"Saudi-Heritage-Lora \"  # @param {type:\"string\"}\n",
        "# @markdown Get your `wandb_api_key` [here](https://wandb.ai/settings) to logs with wandb.\n",
        "wandb_api_key               = \"d9424f672f64cf28774140dac77b19cdce59c305\" # @param {type:\"string\"}\n",
        "in_json                     = \"/content/LoRA/meta_lat.json\"  # @param {type:\"string\"}\n",
        "# @markdown ### **SDXL Config**\n",
        "gradient_checkpointing      = True  # @param {type:\"boolean\"}\n",
        "no_half_vae                 = True  # @param {type:\"boolean\"}\n",
        "#@markdown Recommended parameter for SDXL training but if you enable it, `shuffle_caption` won't work\n",
        "cache_text_encoder_outputs  = False  # @param {type:\"boolean\"}\n",
        "#@markdown These options can be used to train U-Net with different timesteps. The default values are 0 and 1000.\n",
        "min_timestep                = 0 # @param {type:\"number\"}\n",
        "max_timestep                = 1000 # @param {type:\"number\"}\n",
        "# @markdown ### **Dataset Config**\n",
        "num_repeats                 = 1  # @param {type:\"number\"}\n",
        "resolution                  = 768  # @param {type:\"slider\", min:512, max:1024, step:128}\n",
        "keep_tokens                 = 0  # @param {type:\"number\"}\n",
        "# @markdown ### **General Config**\n",
        "num_epochs                  = 10  # @param {type:\"number\"}\n",
        "train_batch_size            = 1  # @param {type:\"number\"}\n",
        "mixed_precision             = \"fp16\"  # @param [\"no\",\"fp16\",\"bf16\"] {allow-input: false}\n",
        "seed                        = -1  # @param {type:\"number\"}\n",
        "optimization                = \"scaled dot-product attention\" # @param [\"xformers\", \"scaled dot-product attention\"]\n",
        "# @markdown ### **Save Output Config**\n",
        "save_precision              = \"fp16\"  # @param [\"float\", \"fp16\", \"bf16\"] {allow-input: false}\n",
        "save_every_n_epochs         = 10  # @param {type:\"number\"}\n",
        "# @markdown ### **Sample Prompt Config**\n",
        "enable_sample               = True  # @param {type:\"boolean\"}\n",
        "sampler                     = \"euler_a\"  # @param [\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\",\"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"]\n",
        "positive_prompt             = \"\"\n",
        "negative_prompt             = \"\"\n",
        "quality_prompt              = \"Stable Diffusion XL\"  # @param [\"None\", \"Waifu Diffusion 1.5\", \"NovelAI\", \"AbyssOrangeMix\", \"Stable Diffusion XL\"] {allow-input: false}\n",
        "if quality_prompt          == \"Waifu Diffusion 1.5\":\n",
        "    positive_prompt         = \"(exceptional, best aesthetic, new, newest, best quality, masterpiece, extremely detailed, anime, waifu:1.2), \"\n",
        "    negative_prompt         = \"lowres, ((bad anatomy)), ((bad hands)), missing finger, extra digits, fewer digits, blurry, ((mutated hands and fingers)), (poorly drawn face), ((mutation)), ((deformed face)), (ugly), ((bad proportions)), ((extra limbs)), extra face, (double head), (extra head), ((extra feet)), monster, logo, cropped, worst quality, jpeg, humpbacked, long body, long neck, ((jpeg artifacts)), deleted, old, oldest, ((censored)), ((bad aesthetic)), (mosaic censoring, bar censor, blur censor), \"\n",
        "if quality_prompt          == \"NovelAI\":\n",
        "    positive_prompt         = \"masterpiece, best quality, \"\n",
        "    negative_prompt         = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, \"\n",
        "if quality_prompt         == \"AbyssOrangeMix\":\n",
        "    positive_prompt         = \"masterpiece, best quality, \"\n",
        "    negative_prompt         = \"(worst quality, low quality:1.4), \"\n",
        "if quality_prompt          == \"Stable Diffusion XL\":\n",
        "    negative_prompt         = \"3d render, smooth, plastic, blurry, grainy, low-resolution, deep-fried, oversaturated\"\n",
        "custom_prompt               = \"\" # @param {type:\"string\"}\n",
        "# @markdown Specify `prompt_from_caption` if you want to use caption as prompt instead. Will be chosen randomly.\n",
        "prompt_from_caption         = \".txt\"  # @param [\"none\", \".txt\", \".caption\"]\n",
        "if prompt_from_caption != \"none\":\n",
        "    custom_prompt           = \"\"\n",
        "num_prompt                  = 2  # @param {type:\"number\"}\n",
        "logging_dir                 = os.path.join(training_dir, \"logs\")\n",
        "lowram                      = True  # @param {type:\"boolean\"}\n",
        "gradient_accumulation_steps = 2  # @param {type:\"number\"}\n",
        "#lowram                      = int(next(line.split()[1] for line in open('/proc/meminfo') if \"MemTotal\" in line)) / (1024**2) < 15\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "\n",
        "prompt_config = {\n",
        "    \"prompt\": {\n",
        "        \"negative_prompt\" : negative_prompt,\n",
        "        \"width\"           : resolution,\n",
        "        \"height\"          : resolution,\n",
        "        \"scale\"           : 12,\n",
        "        \"sample_steps\"    : 28,\n",
        "        \"subset\"          : [],\n",
        "    }\n",
        "}\n",
        "\n",
        "train_config = {\n",
        "    \"sdxl_arguments\": {\n",
        "        \"cache_text_encoder_outputs\" : cache_text_encoder_outputs,\n",
        "        \"no_half_vae\"                : True,\n",
        "        \"min_timestep\"               : min_timestep,\n",
        "        \"max_timestep\"               : max_timestep,\n",
        "        \"shuffle_caption\"            : True if not cache_text_encoder_outputs else False,\n",
        "        \"lowram\"                     : lowram\n",
        "    },\n",
        "    \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\" : model_path,\n",
        "        \"vae\"                           : vae_path,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "        \"debug_dataset\"                 : False,\n",
        "        \"in_json\"                       : in_json,\n",
        "        \"train_data_dir\"                : train_data_dir,\n",
        "        \"dataset_repeats\"               : num_repeats,\n",
        "        \"keep_tokens\"                   : keep_tokens,\n",
        "        \"resolution\"                    : str(resolution) + ',' + str(resolution),\n",
        "        \"color_aug\"                     : False,\n",
        "        \"face_crop_aug_range\"           : None,\n",
        "        \"token_warmup_min\"              : 1,\n",
        "        \"token_warmup_step\"             : 0,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "        \"output_dir\"                    : output_dir,\n",
        "        \"output_name\"                   : project_name if project_name else \"last\",\n",
        "        \"save_precision\"                : save_precision,\n",
        "        \"save_every_n_epochs\"           : save_every_n_epochs,\n",
        "        \"save_n_epoch_ratio\"            : None,\n",
        "        \"save_last_n_epochs\"            : None,\n",
        "        \"resume\"                        : None,\n",
        "        \"train_batch_size\"              : train_batch_size,\n",
        "        \"max_token_length\"              : 225,\n",
        "        \"mem_eff_attn\"                  : False,\n",
        "        \"sdpa\"                          : True if optimization == \"scaled dot-product attention\" else False,\n",
        "        \"xformers\"                      : True if optimization == \"xformers\" else False,\n",
        "        \"max_train_epochs\"              : num_epochs,\n",
        "        \"max_data_loader_n_workers\"     : 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"seed\"                          : seed if seed > 0 else None,\n",
        "        \"gradient_checkpointing\"        : gradient_checkpointing,\n",
        "        \"gradient_accumulation_steps\"   : gradient_accumulation_steps,\n",
        "        \"mixed_precision\"               : mixed_precision,\n",
        "    },\n",
        "    \"logging_arguments\": {\n",
        "        \"log_with\"          : \"wandb\" if wandb_api_key else \"tensorboard\",\n",
        "        \"log_tracker_name\"  : project_name if wandb_api_key and not project_name == \"last\" else None,\n",
        "        \"logging_dir\"       : logging_dir,\n",
        "        \"log_prefix\"        : project_name if not wandb_api_key else None,\n",
        "    },\n",
        "    \"sample_prompt_arguments\": {\n",
        "        \"sample_every_n_steps\"    : None,\n",
        "        \"sample_every_n_epochs\"   : save_every_n_epochs if enable_sample else None,\n",
        "        \"sample_sampler\"          : sampler,\n",
        "    },\n",
        "    \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\"\n",
        "    },\n",
        "}\n",
        "\n",
        "def write_file(filename, contents):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "def prompt_convert(enable_sample, num_prompt, train_data_dir, prompt_config, custom_prompt):\n",
        "    if enable_sample:\n",
        "        search_pattern = os.path.join(train_data_dir, '**/*' + prompt_from_caption)\n",
        "        caption_files = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "        if not caption_files:\n",
        "            if not custom_prompt:\n",
        "                custom_prompt = \"masterpiece, best quality, 1girl, aqua eyes, baseball cap, blonde hair, closed mouth, earrings, green background, hat, hoop earrings, jewelry, looking at viewer, shirt, short hair, simple background, solo, upper body, yellow shirt\"\n",
        "            new_prompt_config = prompt_config.copy()\n",
        "            new_prompt_config['prompt']['subset'] = [\n",
        "                {\"prompt\": positive_prompt + custom_prompt if positive_prompt else custom_prompt}\n",
        "            ]\n",
        "        else:\n",
        "            selected_files = random.sample(caption_files, min(num_prompt, len(caption_files)))\n",
        "\n",
        "            prompts = []\n",
        "            for file in selected_files:\n",
        "                with open(file, 'r') as f:\n",
        "                    prompts.append(f.read().strip())\n",
        "\n",
        "            new_prompt_config = prompt_config.copy()\n",
        "            new_prompt_config['prompt']['subset'] = []\n",
        "\n",
        "            for prompt in prompts:\n",
        "                new_prompt = {\n",
        "                    \"prompt\": positive_prompt + prompt if positive_prompt else prompt,\n",
        "                }\n",
        "                new_prompt_config['prompt']['subset'].append(new_prompt)\n",
        "\n",
        "        return new_prompt_config\n",
        "    else:\n",
        "        return prompt_config\n",
        "\n",
        "def eliminate_none_variable(config):\n",
        "    for key in config:\n",
        "        if isinstance(config[key], dict):\n",
        "            for sub_key in config[key]:\n",
        "                if config[key][sub_key] == \"\":\n",
        "                    config[key][sub_key] = None\n",
        "        elif config[key] == \"\":\n",
        "            config[key] = None\n",
        "\n",
        "    return config\n",
        "\n",
        "try:\n",
        "    train_config.update(optimizer_config)\n",
        "except NameError:\n",
        "    raise NameError(\"'optimizer_config' dictionary is missing. Please run  '4.1. Optimizer Config' cell.\")\n",
        "\n",
        "try:\n",
        "    train_config.update(lora_config)\n",
        "except NameError:\n",
        "    raise NameError(\"'lora_config' dictionary is missing. Please run  '4.1. LoRa: Low-Rank Adaptation Config' cell.\")\n",
        "\n",
        "advanced_training_warning = False\n",
        "try:\n",
        "    train_config.update(advanced_training_config)\n",
        "except NameError:\n",
        "    advanced_training_warning = True\n",
        "    pass\n",
        "\n",
        "prompt_config = prompt_convert(enable_sample, num_prompt, train_data_dir, prompt_config, custom_prompt)\n",
        "\n",
        "config_path         = os.path.join(config_dir, \"config_file.toml\")\n",
        "prompt_path         = os.path.join(config_dir, \"sample_prompt.toml\")\n",
        "\n",
        "config_str          = toml.dumps(eliminate_none_variable(train_config))\n",
        "prompt_str          = toml.dumps(eliminate_none_variable(prompt_config))\n",
        "\n",
        "write_file(config_path, config_str)\n",
        "write_file(prompt_path, prompt_str)\n",
        "\n",
        "print(config_str)\n",
        "\n",
        "if advanced_training_warning:\n",
        "    import textwrap\n",
        "    error_message = \"WARNING: This is not an error message, but the [advanced_training_config] dictionary is missing. Please run the '4.2. Advanced Training Config' cell if you intend to use it, or continue to the next step.\"\n",
        "    wrapped_message = textwrap.fill(error_message, width=80)\n",
        "    print('\\033[38;2;204;102;102m' + wrapped_message + '\\033[0m\\n')\n",
        "    pass\n",
        "\n",
        "print(prompt_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "p_SHtbFwHVl1",
        "outputId": "0e58e09d-9b9b-42b5-b3d8-b46939ddbed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751060928.071648   12576 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751060928.094391   12576 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751060935.790612   12620 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751060935.804136   12620 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading settings from /content/LoRA/config/config_file.toml...\n",
            "/content/LoRA/config/config_file\n",
            "prepare tokenizers\n",
            "Downloading vocab.json: 961kB [00:00, 23.9MB/s]\n",
            "Downloading merges.txt: 525kB [00:00, 54.5MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 389/389 [00:00<00:00, 2.07MB/s]\n",
            "Downloading tokenizer_config.json: 100% 905/905 [00:00<00:00, 4.95MB/s]\n",
            "Downloading vocab.json: 862kB [00:00, 80.3MB/s]\n",
            "Downloading merges.txt: 525kB [00:00, 86.4MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 389/389 [00:00<00:00, 3.12MB/s]\n",
            "Downloading tokenizer_config.json: 100% 904/904 [00:00<00:00, 6.69MB/s]\n",
            "update token length: 225\n",
            "Training with captions.\n",
            "loading existing metadata: /content/LoRA/meta_lat.json\n",
            "metadata has bucket info, enable bucketing / メタデータにbucket情報があるためbucketを有効にします\n",
            "using bucket info in metadata / メタデータ内のbucket情報を使います\n",
            "[Dataset 0]\n",
            "  batch_size: 1\n",
            "  resolution: (768, 768)\n",
            "  enable_bucket: True\n",
            "  min_bucket_reso: None\n",
            "  max_bucket_reso: None\n",
            "  bucket_reso_steps: None\n",
            "  bucket_no_upscale: None\n",
            "\n",
            "  [Subset 0 of Dataset 0]\n",
            "    image_dir: \"/content/drive/MyDrive/Saudi-Heritage-GenAI/saudi-heritage-dataset\"\n",
            "    image_count: 384\n",
            "    num_repeats: 1\n",
            "    shuffle_caption: True\n",
            "    keep_tokens: 0\n",
            "    caption_dropout_rate: 0\n",
            "    caption_dropout_every_n_epoches: 0\n",
            "    caption_tag_dropout_rate: 0\n",
            "    color_aug: False\n",
            "    flip_aug: False\n",
            "    face_crop_aug_range: None\n",
            "    random_crop: False\n",
            "    token_warmup_min: 1,\n",
            "    token_warmup_step: 0,\n",
            "    metadata_file: /content/LoRA/meta_lat.json\n",
            "\n",
            "\n",
            "[Dataset 0]\n",
            "loading image sizes.\n",
            "100% 384/384 [00:00<00:00, 3151884.02it/s]\n",
            "make buckets\n",
            "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
            "bucket 0: resolution (256, 1024), count: 2\n",
            "bucket 1: resolution (320, 1024), count: 12\n",
            "bucket 2: resolution (384, 1024), count: 11\n",
            "bucket 3: resolution (448, 1024), count: 8\n",
            "bucket 4: resolution (512, 1024), count: 9\n",
            "bucket 5: resolution (576, 1024), count: 17\n",
            "bucket 6: resolution (640, 1024), count: 14\n",
            "bucket 7: resolution (704, 1024), count: 33\n",
            "bucket 8: resolution (768, 1024), count: 31\n",
            "bucket 9: resolution (832, 1024), count: 144\n",
            "bucket 10: resolution (896, 1024), count: 18\n",
            "bucket 11: resolution (960, 1024), count: 9\n",
            "bucket 12: resolution (1024, 640), count: 1\n",
            "bucket 13: resolution (1024, 704), count: 8\n",
            "bucket 14: resolution (1024, 768), count: 4\n",
            "bucket 15: resolution (1024, 832), count: 4\n",
            "bucket 16: resolution (1024, 896), count: 4\n",
            "bucket 17: resolution (1024, 960), count: 11\n",
            "bucket 18: resolution (1024, 1024), count: 44\n",
            "mean ar error (without repeats): 0.0\n",
            "noise_offset is set to 0.0357 / noise_offsetが0.0357に設定されました\n",
            "preparing accelerator\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "loading model for process 0/1\n",
            "load Diffusers pretrained models: stabilityai/stable-diffusion-xl-base-1.0, variant=fp16\n",
            "Downloading model_index.json: 100% 609/609 [00:00<00:00, 4.55MB/s]\n",
            "Fetching 23 files:   0% 0/23 [00:00<?, ?it/s]\n",
            "Downloading config.json: 100% 565/565 [00:00<00:00, 3.88MB/s]\n",
            "\n",
            "Downloading scheduler_config.json: 100% 479/479 [00:00<00:00, 4.43MB/s]\n",
            "Fetching 23 files:   9% 2/23 [00:00<00:01, 13.91it/s]\n",
            "Downloading merges.txt: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "Downloading config.json: 100% 575/575 [00:00<00:00, 4.37MB/s]\n",
            "\n",
            "\n",
            "Downloading model.onnx:   0% 0.00/1.04M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:   0% 0.00/246M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading merges.txt: 525kB [00:00, 15.7MB/s]\n",
            "\n",
            "Downloading model.onnx: 100% 1.04M/1.04M [00:00<00:00, 27.8MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)cial_tokens_map.json: 100% 460/460 [00:00<00:00, 2.18MB/s]\n",
            "\n",
            "\n",
            "Downloading tokenizer_config.json: 100% 725/725 [00:00<00:00, 1.12MB/s]\n",
            "\n",
            "\n",
            "Downloading vocab.json: 1.06MB [00:00, 33.0MB/s]\n",
            "\n",
            "\n",
            "Downloading config.json: 1.68kB [00:00, 6.51MB/s]\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   0% 0.00/5.14G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   1% 10.5M/1.39G [00:00<00:19, 70.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:   4% 10.5M/246M [00:00<00:03, 59.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   2% 10.5M/493M [00:00<00:08, 53.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading config.json: 100% 642/642 [00:00<00:00, 2.79MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading config.json: 100% 607/607 [00:00<00:00, 2.36MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   0% 0.00/7.29M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   0% 0.00/167M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   0% 10.5M/5.14G [00:00<01:26, 59.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx: 100% 7.29M/7.29M [00:00<00:00, 64.9MB/s]\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:   9% 21.0M/246M [00:00<00:04, 53.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   4% 21.0M/493M [00:00<00:08, 53.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   6% 10.5M/167M [00:00<00:02, 72.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   2% 31.5M/1.39G [00:00<00:17, 79.2MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   0% 21.0M/5.14G [00:00<01:23, 61.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  13% 31.5M/246M [00:00<00:03, 60.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   6% 31.5M/493M [00:00<00:08, 55.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  13% 21.0M/167M [00:00<00:02, 61.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   3% 41.9M/1.39G [00:00<00:20, 66.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   0% 0.00/198M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   0% 0.00/167M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   1% 31.5M/5.14G [00:00<01:36, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  19% 31.5M/167M [00:00<00:01, 68.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   9% 41.9M/493M [00:00<00:07, 57.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  17% 41.9M/246M [00:00<00:03, 53.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   5% 10.5M/198M [00:00<00:03, 52.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   4% 52.4M/1.39G [00:00<00:23, 56.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   0% 0.00/137M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  11% 52.4M/493M [00:00<00:07, 56.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   6% 10.5M/167M [00:00<00:04, 32.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  25% 41.9M/167M [00:00<00:02, 55.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   1% 41.9M/5.14G [00:00<01:50, 46.0MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   5% 62.9M/1.39G [00:01<00:23, 57.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  21% 52.4M/246M [00:01<00:08, 23.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  11% 21.0M/198M [00:01<00:15, 11.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:   8% 10.5M/137M [00:01<00:16, 7.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   1% 52.4M/5.14G [00:02<04:56, 17.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  13% 62.9M/493M [00:02<00:24, 17.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   5% 73.4M/1.39G [00:02<01:06, 19.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  31% 52.4M/167M [00:02<00:06, 17.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  13% 21.0M/167M [00:01<00:13, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  26% 62.9M/246M [00:02<00:09, 19.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   1% 62.9M/5.14G [00:02<03:37, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  15% 21.0M/137M [00:01<00:07, 15.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  15% 73.4M/493M [00:02<00:18, 22.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   6% 83.9M/1.39G [00:02<00:54, 24.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  16% 31.5M/198M [00:01<00:10, 16.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  30% 73.4M/246M [00:02<00:07, 24.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  38% 62.9M/167M [00:02<00:04, 21.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  23% 31.5M/137M [00:01<00:04, 21.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   1% 73.4M/5.14G [00:02<03:15, 26.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  19% 31.5M/167M [00:02<00:08, 15.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  21% 41.9M/198M [00:02<00:07, 21.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  44% 73.4M/167M [00:02<00:03, 25.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   7% 94.4M/1.39G [00:02<00:47, 27.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  17% 83.9M/493M [00:02<00:16, 24.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  34% 83.9M/246M [00:02<00:05, 27.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  31% 41.9M/137M [00:02<00:03, 29.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  25% 41.9M/167M [00:02<00:06, 20.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   2% 83.9M/5.14G [00:02<02:53, 29.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  26% 52.4M/198M [00:02<00:05, 27.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  50% 83.9M/167M [00:02<00:02, 29.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   8% 105M/1.39G [00:03<00:43, 29.8MB/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  31% 52.4M/167M [00:02<00:04, 27.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  38% 94.4M/246M [00:03<00:05, 29.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  19% 94.4M/493M [00:03<00:14, 27.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  32% 62.9M/198M [00:02<00:04, 32.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  56% 94.4M/167M [00:03<00:02, 35.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  38% 52.4M/137M [00:02<00:02, 28.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   2% 94.4M/5.14G [00:03<02:40, 31.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  38% 62.9M/167M [00:02<00:03, 32.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   8% 115M/1.39G [00:03<00:38, 32.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  21% 105M/493M [00:03<00:12, 31.2MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  43% 105M/246M [00:03<00:04, 33.2MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  37% 73.4M/198M [00:02<00:03, 38.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  63% 105M/167M [00:03<00:01, 40.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  46% 62.9M/137M [00:02<00:02, 34.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   2% 105M/5.14G [00:03<02:49, 29.7MB/s] \u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:   9% 126M/1.39G [00:05<01:50, 11.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   2% 115M/5.14G [00:05<06:48, 12.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  42% 83.9M/198M [00:05<00:09, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  69% 115M/167M [00:05<00:04, 12.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  44% 73.4M/167M [00:05<00:08, 10.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  23% 115M/493M [00:05<00:34, 11.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  47% 115M/246M [00:05<00:11, 11.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  54% 73.4M/137M [00:04<00:05, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  10% 136M/1.39G [00:05<01:24, 14.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   2% 126M/5.14G [00:05<05:16, 15.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  48% 94.4M/198M [00:05<00:06, 14.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  75% 126M/167M [00:05<00:02, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  50% 83.9M/167M [00:05<00:06, 13.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  51% 126M/246M [00:06<00:08, 14.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  61% 83.9M/137M [00:05<00:03, 14.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  11% 147M/1.39G [00:06<01:05, 18.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  26% 126M/493M [00:06<00:27, 13.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   3% 136M/5.14G [00:06<04:16, 19.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  53% 105M/198M [00:05<00:04, 18.9MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  55% 136M/246M [00:06<00:06, 18.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  56% 94.4M/167M [00:05<00:04, 17.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  81% 136M/167M [00:05<00:01, 18.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  69% 94.4M/137M [00:05<00:02, 17.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  11% 157M/1.39G [00:06<00:54, 22.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  28% 136M/493M [00:06<00:20, 17.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   3% 147M/5.14G [00:06<03:32, 23.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  58% 115M/198M [00:05<00:03, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  88% 147M/167M [00:06<00:00, 22.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  60% 147M/246M [00:06<00:04, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  63% 105M/167M [00:05<00:03, 20.5MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  30% 147M/493M [00:06<00:16, 21.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  77% 105M/137M [00:05<00:01, 21.2MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  12% 168M/1.39G [00:06<00:46, 26.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  94% 157M/167M [00:06<00:00, 28.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   3% 157M/5.14G [00:06<03:06, 26.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  64% 126M/198M [00:06<00:02, 26.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  64% 157M/246M [00:06<00:03, 25.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  69% 115M/167M [00:06<00:02, 25.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  32% 157M/493M [00:06<00:13, 25.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  84% 115M/137M [00:05<00:00, 25.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  13% 178M/1.39G [00:06<00:44, 27.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   3% 168M/5.14G [00:06<02:56, 28.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  34% 168M/493M [00:06<00:10, 30.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  69% 136M/198M [00:06<00:02, 27.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  68% 168M/246M [00:07<00:02, 28.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  75% 126M/167M [00:06<00:01, 28.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors: 100% 167M/167M [00:06<00:00, 27.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors: 100% 167M/167M [00:06<00:00, 24.2MB/s]\n",
            "\n",
            "Downloading model.fp16.safetensors:  14% 189M/1.39G [00:07<00:41, 29.2MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   3% 178M/5.14G [00:07<02:42, 30.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  74% 147M/198M [00:06<00:01, 30.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  36% 178M/493M [00:07<00:10, 31.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  72% 178M/246M [00:07<00:02, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  81% 136M/167M [00:06<00:01, 29.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   4% 189M/5.14G [00:07<02:13, 36.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx: 100% 136M/137M [00:06<00:00, 30.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  14% 199M/1.39G [00:07<00:35, 33.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  79% 157M/198M [00:06<00:01, 35.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   4% 199M/5.14G [00:07<01:50, 44.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  38% 189M/493M [00:07<00:09, 32.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  77% 189M/246M [00:07<00:02, 28.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.onnx: 100% 137M/137M [00:06<00:00, 19.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  88% 147M/167M [00:07<00:00, 25.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  40% 199M/493M [00:07<00:09, 32.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  85% 168M/198M [00:07<00:01, 28.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  15% 210M/1.39G [00:07<00:42, 27.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  81% 199M/246M [00:07<00:01, 30.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   4% 220M/5.14G [00:08<04:32, 18.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  43% 210M/493M [00:09<00:16, 17.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  16% 220M/1.39G [00:09<01:11, 16.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  85% 210M/246M [00:09<00:02, 16.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  94% 157M/167M [00:08<00:00, 14.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  90% 178M/198M [00:08<00:01, 15.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   4% 231M/5.14G [00:09<03:43, 22.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   5% 241M/5.14G [00:10<05:23, 15.1MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  17% 231M/1.39G [00:10<01:34, 12.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  45% 220M/493M [00:10<00:22, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  89% 220M/246M [00:10<00:02, 12.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   5% 252M/5.14G [00:10<04:02, 20.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  47% 231M/493M [00:10<00:16, 16.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  95% 189M/198M [00:10<00:00, 11.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  94% 231M/246M [00:10<00:00, 16.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)del.fp16.safetensors: 100% 167M/167M [00:10<00:00, 16.3MB/s]\n",
            "\n",
            "Downloading model.fp16.safetensors:  17% 241M/1.39G [00:11<01:52, 10.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  49% 241M/493M [00:12<00:20, 12.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors:  98% 241M/246M [00:12<00:00, 12.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   5% 262M/5.14G [00:11<06:06, 13.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx: 100% 198M/198M [00:11<00:00, 17.2MB/s]\n",
            "\n",
            "Downloading model.fp16.safetensors:  18% 252M/1.39G [00:12<01:23, 13.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.fp16.safetensors: 100% 246M/246M [00:12<00:00, 20.2MB/s]\n",
            "Fetching 23 files:  17% 4/23 [00:12<01:08,  3.63s/it]\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   5% 273M/5.14G [00:12<04:33, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   6% 294M/5.14G [00:13<04:37, 17.4MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  19% 262M/1.39G [00:13<01:39, 11.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  53% 262M/493M [00:13<00:18, 12.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   6% 304M/5.14G [00:13<03:41, 21.9MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  20% 273M/1.39G [00:13<01:13, 15.3MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   6% 325M/5.14G [00:13<02:20, 34.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  55% 273M/493M [00:13<00:13, 16.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   7% 336M/5.14G [00:13<01:58, 40.4MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  21% 294M/1.39G [00:13<00:44, 24.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  57% 283M/493M [00:13<00:09, 21.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  60% 294M/493M [00:14<00:08, 23.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  22% 304M/1.39G [00:14<00:43, 25.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   7% 357M/5.14G [00:14<01:52, 42.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  62% 304M/493M [00:14<00:06, 29.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   7% 367M/5.14G [00:14<01:39, 48.0MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  23% 315M/1.39G [00:14<00:35, 30.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  64% 315M/493M [00:14<00:04, 35.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  23% 325M/1.39G [00:14<00:29, 36.2MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   8% 388M/5.14G [00:14<01:16, 62.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  66% 325M/493M [00:14<00:03, 43.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   8% 398M/5.14G [00:14<01:12, 65.5MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  24% 336M/1.39G [00:14<00:25, 41.2MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   8% 409M/5.14G [00:14<01:12, 64.9MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  26% 357M/1.39G [00:14<00:18, 56.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  70% 346M/493M [00:14<00:02, 54.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  26% 367M/1.39G [00:14<00:16, 60.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  72% 357M/493M [00:15<00:02, 57.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   8% 430M/5.14G [00:14<01:02, 75.6MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  27% 377M/1.39G [00:15<00:15, 65.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  77% 377M/493M [00:15<00:01, 68.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   9% 440M/5.14G [00:15<01:12, 65.2MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  28% 388M/1.39G [00:15<00:15, 63.2MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  29% 398M/1.39G [00:15<00:13, 70.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  81% 398M/493M [00:15<00:01, 80.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   9% 461M/5.14G [00:15<00:59, 78.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  83% 409M/493M [00:15<00:01, 81.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   9% 472M/5.14G [00:15<01:01, 75.8MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  30% 419M/1.39G [00:15<00:13, 70.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  85% 419M/493M [00:15<00:00, 76.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:   9% 482M/5.14G [00:15<01:03, 73.7MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  31% 430M/1.39G [00:15<00:13, 68.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  89% 440M/493M [00:15<00:00, 91.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  10% 503M/5.14G [00:15<01:04, 72.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  92% 451M/493M [00:16<00:00, 77.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  32% 451M/1.39G [00:16<00:12, 73.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  10% 514M/5.14G [00:16<01:15, 61.3MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  33% 461M/1.39G [00:17<00:38, 24.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  94% 461M/493M [00:17<00:01, 23.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  10% 524M/5.14G [00:17<03:13, 23.8MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  34% 472M/1.39G [00:17<00:31, 29.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  96% 472M/493M [00:17<00:00, 27.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  10% 535M/5.14G [00:17<02:40, 28.6MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  35% 482M/1.39G [00:18<00:50, 18.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  11% 545M/5.14G [00:18<04:24, 17.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx:  98% 482M/493M [00:20<00:00, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  35% 493M/1.39G [00:21<01:26, 10.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  11% 556M/5.14G [00:22<09:47, 7.80MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading model.onnx: 100% 493M/493M [00:22<00:00, 8.26MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Downloading model.onnx: 100% 493M/493M [00:22<00:00, 22.0MB/s]\n",
            "Fetching 23 files:  22% 5/23 [00:22<01:40,  5.57s/it]\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  11% 566M/5.14G [00:23<09:30, 8.02MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  37% 514M/1.39G [00:23<01:34, 9.30MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  11% 577M/5.14G [00:23<07:01, 10.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  11% 587M/5.14G [00:24<07:44, 9.80MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  38% 535M/1.39G [00:24<01:15, 11.3MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  12% 598M/5.14G [00:24<05:40, 13.3MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  40% 556M/1.39G [00:25<00:47, 17.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  12% 619M/5.14G [00:25<03:19, 22.6MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  42% 577M/1.39G [00:25<00:31, 25.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  12% 640M/5.14G [00:25<02:13, 33.7MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  43% 598M/1.39G [00:25<00:22, 35.7MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  45% 619M/1.39G [00:25<00:21, 36.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  13% 661M/5.14G [00:25<02:10, 34.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  13% 682M/5.14G [00:25<01:33, 47.8MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  46% 640M/1.39G [00:26<00:15, 48.7MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  48% 661M/1.39G [00:26<00:11, 62.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  14% 713M/5.14G [00:26<01:02, 70.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  14% 734M/5.14G [00:26<00:52, 83.1MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  49% 682M/1.39G [00:26<00:09, 72.9MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  51% 703M/1.39G [00:26<00:07, 89.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  15% 755M/5.14G [00:26<00:46, 93.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  15% 776M/5.14G [00:26<00:39, 111MB/s] \u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  52% 724M/1.39G [00:26<00:06, 103MB/s] \u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  16% 797M/5.14G [00:26<00:34, 126MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  54% 744M/1.39G [00:26<00:05, 115MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  16% 818M/5.14G [00:26<00:30, 140MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  55% 765M/1.39G [00:26<00:04, 127MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  16% 839M/5.14G [00:26<00:29, 147MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  57% 786M/1.39G [00:27<00:04, 133MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  17% 860M/5.14G [00:28<01:41, 42.3MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  58% 807M/1.39G [00:28<00:14, 39.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  17% 881M/5.14G [00:28<01:22, 51.8MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  60% 828M/1.39G [00:28<00:10, 51.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  18% 902M/5.14G [00:28<01:04, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  18% 923M/5.14G [00:29<02:05, 33.6MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  61% 849M/1.39G [00:30<00:18, 28.7MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  18% 944M/5.14G [00:29<01:34, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  19% 965M/5.14G [00:32<03:12, 21.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  19% 996M/5.14G [00:32<02:02, 33.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  20% 1.03G/5.14G [00:32<01:22, 49.5MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  63% 870M/1.39G [00:32<00:30, 17.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  20% 1.05G/5.14G [00:32<01:08, 59.9MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  64% 891M/1.39G [00:32<00:21, 22.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  21% 1.07G/5.14G [00:32<00:58, 69.7MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  65% 902M/1.39G [00:32<00:19, 25.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  21% 1.09G/5.14G [00:32<00:51, 77.9MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  66% 912M/1.39G [00:32<00:15, 30.1MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  67% 933M/1.39G [00:33<00:10, 42.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  22% 1.11G/5.14G [00:32<00:46, 86.7MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  69% 954M/1.39G [00:33<00:07, 56.1MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  22% 1.13G/5.14G [00:33<00:39, 102MB/s] \u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  70% 975M/1.39G [00:33<00:05, 70.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  22% 1.15G/5.14G [00:33<00:35, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  23% 1.17G/5.14G [00:33<00:32, 121MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  72% 996M/1.39G [00:33<00:04, 81.5MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  23% 1.20G/5.14G [00:33<00:34, 113MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  73% 1.02G/1.39G [00:33<00:04, 85.4MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  24% 1.22G/5.14G [00:33<00:34, 112MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  75% 1.04G/1.39G [00:33<00:03, 89.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  24% 1.24G/5.14G [00:33<00:36, 107MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  76% 1.06G/1.39G [00:34<00:03, 91.1MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  78% 1.08G/1.39G [00:35<00:08, 36.6MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  25% 1.26G/5.14G [00:35<01:43, 37.4MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  78% 1.09G/1.39G [00:35<00:07, 40.9MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  25% 1.27G/5.14G [00:35<01:33, 41.3MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  79% 1.10G/1.39G [00:35<00:06, 46.3MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  25% 1.28G/5.14G [00:35<01:22, 46.7MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  80% 1.11G/1.39G [00:35<00:05, 52.8MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  25% 1.30G/5.14G [00:35<01:05, 58.6MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  82% 1.13G/1.39G [00:35<00:03, 69.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  26% 1.31G/5.14G [00:35<01:00, 62.8MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  83% 1.15G/1.39G [00:36<00:02, 80.0MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  26% 1.33G/5.14G [00:36<00:48, 78.1MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  85% 1.17G/1.39G [00:36<00:02, 95.7MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  86% 1.20G/1.39G [00:36<00:01, 109MB/s] \u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  26% 1.34G/5.14G [00:36<00:58, 64.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  26% 1.35G/5.14G [00:36<00:53, 70.4MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  88% 1.22G/1.39G [00:36<00:01, 118MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  89% 1.24G/1.39G [00:36<00:01, 124MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  27% 1.37G/5.14G [00:36<00:43, 87.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  27% 1.39G/5.14G [00:36<00:36, 101MB/s] \u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  91% 1.26G/1.39G [00:36<00:01, 127MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  28% 1.42G/5.14G [00:36<00:31, 119MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  92% 1.28G/1.39G [00:37<00:00, 135MB/s]\u001b[A\n",
            "Downloading model.fp16.safetensors:  94% 1.30G/1.39G [00:37<00:00, 151MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  28% 1.44G/5.14G [00:37<00:30, 122MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  95% 1.32G/1.39G [00:37<00:00, 157MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  28% 1.46G/5.14G [00:37<00:27, 134MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  29% 1.49G/5.14G [00:37<00:21, 166MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  97% 1.34G/1.39G [00:37<00:00, 120MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  30% 1.52G/5.14G [00:37<00:20, 180MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors:  98% 1.36G/1.39G [00:37<00:00, 116MB/s]\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  30% 1.54G/5.14G [00:37<00:24, 149MB/s]\u001b[A\u001b[A\n",
            "Downloading model.fp16.safetensors: 100% 1.38G/1.39G [00:37<00:00, 119MB/s]\u001b[A\n",
            "\n",
            "Downloading model.fp16.safetensors: 100% 1.39G/1.39G [00:37<00:00, 36.6MB/s]\n",
            "Fetching 23 files:  30% 7/23 [00:38<01:45,  6.58s/it]\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  31% 1.59G/5.14G [00:37<00:19, 182MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  32% 1.63G/5.14G [00:38<00:17, 203MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  32% 1.66G/5.14G [00:38<00:18, 188MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  33% 1.69G/5.14G [00:38<00:16, 208MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  33% 1.72G/5.14G [00:38<00:16, 212MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  34% 1.75G/5.14G [00:38<00:15, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  35% 1.78G/5.14G [00:38<00:14, 232MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  35% 1.81G/5.14G [00:38<00:13, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  36% 1.85G/5.14G [00:38<00:12, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  37% 1.88G/5.14G [00:39<00:12, 263MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  37% 1.91G/5.14G [00:39<00:11, 270MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  38% 1.94G/5.14G [00:39<00:11, 279MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  38% 1.97G/5.14G [00:39<00:13, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  39% 2.00G/5.14G [00:42<01:26, 36.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  39% 2.02G/5.14G [00:42<01:16, 40.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  40% 2.06G/5.14G [00:42<00:54, 56.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  41% 2.09G/5.14G [00:42<00:40, 74.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  41% 2.12G/5.14G [00:43<01:09, 43.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  42% 2.14G/5.14G [00:44<00:57, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  42% 2.16G/5.14G [00:44<00:46, 63.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  43% 2.19G/5.14G [00:44<00:34, 86.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  43% 2.22G/5.14G [00:44<00:26, 111MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  44% 2.25G/5.14G [00:44<00:21, 137MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  45% 2.29G/5.14G [00:44<00:17, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  45% 2.32G/5.14G [00:44<00:15, 185MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  46% 2.35G/5.14G [00:44<00:14, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  46% 2.38G/5.14G [00:45<00:12, 219MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  47% 2.41G/5.14G [00:45<00:11, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  48% 2.44G/5.14G [00:45<00:11, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  48% 2.47G/5.14G [00:45<00:10, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  49% 2.51G/5.14G [00:45<00:10, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  49% 2.54G/5.14G [00:45<00:10, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  50% 2.57G/5.14G [00:45<00:10, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  51% 2.60G/5.14G [00:45<00:10, 231MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  51% 2.63G/5.14G [00:46<00:11, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  52% 2.66G/5.14G [00:46<00:10, 239MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  52% 2.69G/5.14G [00:46<00:10, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  53% 2.73G/5.14G [00:46<00:10, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  54% 2.76G/5.14G [00:46<00:10, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  54% 2.79G/5.14G [00:46<00:09, 250MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  55% 2.82G/5.14G [00:46<00:09, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  56% 2.85G/5.14G [00:46<00:09, 229MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  56% 2.88G/5.14G [00:47<00:09, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  57% 2.92G/5.14G [00:47<00:09, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  57% 2.95G/5.14G [00:47<00:13, 159MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  58% 2.97G/5.14G [00:47<00:12, 168MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  58% 2.99G/5.14G [00:47<00:12, 170MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  59% 3.01G/5.14G [00:47<00:12, 177MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  59% 3.04G/5.14G [00:48<00:10, 195MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  60% 3.07G/5.14G [00:48<00:09, 211MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  60% 3.10G/5.14G [00:48<00:09, 222MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  61% 3.14G/5.14G [00:48<00:09, 217MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  62% 3.17G/5.14G [00:48<00:10, 191MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  62% 3.20G/5.14G [00:48<00:10, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  63% 3.22G/5.14G [00:54<02:07, 15.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  63% 3.25G/5.14G [00:54<01:26, 21.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  64% 3.29G/5.14G [00:54<00:54, 33.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  65% 3.32G/5.14G [00:54<00:39, 45.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  65% 3.36G/5.14G [00:55<00:29, 59.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  66% 3.39G/5.14G [00:55<00:22, 77.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  67% 3.42G/5.14G [00:55<00:17, 99.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  67% 3.45G/5.14G [00:55<00:13, 122MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  68% 3.48G/5.14G [00:55<00:11, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  68% 3.51G/5.14G [00:55<00:09, 173MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  69% 3.54G/5.14G [00:55<00:09, 163MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  70% 3.58G/5.14G [00:56<00:13, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  70% 3.60G/5.14G [01:04<02:24, 10.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  70% 3.61G/5.14G [01:16<02:23, 10.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  70% 3.62G/5.14G [01:29<08:58, 2.82MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  71% 3.63G/5.14G [01:35<09:57, 2.52MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  71% 3.65G/5.14G [01:36<06:59, 3.54MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  71% 3.66G/5.14G [01:36<05:47, 4.25MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  71% 3.67G/5.14G [01:36<04:40, 5.23MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  72% 3.68G/5.14G [01:36<03:41, 6.56MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  72% 3.70G/5.14G [01:36<02:17, 10.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  72% 3.71G/5.14G [01:36<01:51, 12.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  72% 3.72G/5.14G [01:37<01:28, 16.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  73% 3.74G/5.14G [01:37<00:55, 25.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  73% 3.76G/5.14G [01:37<00:37, 36.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  74% 3.79G/5.14G [01:37<00:26, 51.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  74% 3.81G/5.14G [01:37<00:19, 67.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  75% 3.84G/5.14G [01:37<00:13, 95.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  75% 3.87G/5.14G [01:37<00:10, 121MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  76% 3.90G/5.14G [01:37<00:08, 141MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  76% 3.92G/5.14G [01:38<00:08, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  77% 3.94G/5.14G [01:38<00:07, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  77% 3.96G/5.14G [01:38<00:07, 161MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  78% 3.98G/5.14G [01:38<00:09, 124MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  78% 4.01G/5.14G [01:43<01:29, 12.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  79% 4.04G/5.14G [01:44<00:55, 19.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  79% 4.06G/5.14G [01:44<00:41, 26.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  80% 4.09G/5.14G [01:44<00:26, 38.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  80% 4.11G/5.14G [01:44<00:20, 49.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  81% 4.14G/5.14G [01:44<00:14, 69.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  81% 4.17G/5.14G [01:44<00:10, 91.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  82% 4.20G/5.14G [01:44<00:07, 118MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  82% 4.24G/5.14G [01:44<00:06, 141MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  83% 4.27G/5.14G [01:44<00:05, 163MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  84% 4.30G/5.14G [01:46<00:15, 55.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  84% 4.32G/5.14G [01:46<00:12, 65.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  85% 4.34G/5.14G [01:46<00:10, 78.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  85% 4.37G/5.14G [01:46<00:07, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  86% 4.40G/5.14G [01:46<00:05, 126MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  86% 4.44G/5.14G [01:47<00:04, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  87% 4.47G/5.14G [01:47<00:04, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  88% 4.50G/5.14G [01:47<00:03, 186MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  88% 4.53G/5.14G [01:47<00:02, 207MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  89% 4.56G/5.14G [01:47<00:02, 216MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  89% 4.59G/5.14G [01:47<00:02, 225MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  90% 4.62G/5.14G [01:47<00:02, 232MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  91% 4.66G/5.14G [01:47<00:01, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  91% 4.69G/5.14G [01:48<00:01, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  92% 4.72G/5.14G [01:48<00:02, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  93% 4.75G/5.14G [01:48<00:02, 179MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  93% 4.78G/5.14G [01:48<00:01, 198MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  94% 4.81G/5.14G [01:48<00:01, 220MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  94% 4.84G/5.14G [01:48<00:01, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  95% 4.88G/5.14G [01:48<00:01, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  96% 4.91G/5.14G [01:49<00:00, 243MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  96% 4.94G/5.14G [01:49<00:00, 242MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  97% 4.97G/5.14G [01:49<00:00, 236MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  98% 5.01G/5.14G [01:49<00:00, 247MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  98% 5.04G/5.14G [01:49<00:00, 237MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  99% 5.08G/5.14G [01:49<00:00, 248MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors:  99% 5.11G/5.14G [01:49<00:00, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading (…)del.fp16.safetensors: 100% 5.14G/5.14G [01:50<00:00, 46.6MB/s]\n",
            "Fetching 23 files: 100% 23/23 [01:50<00:00,  4.81s/it]\n",
            "The config attributes {'add_watermarker': None} were passed to StableDiffusionXLPipeline, but are not expected and will be ignored. Please verify your model_index.json configuration file.\n",
            "Keyword arguments {'add_watermarker': None} are not expected by StableDiffusionXLPipeline and will be ignored.\n",
            "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
            "U-Net converted to original U-Net\n",
            "Enable SDPA for U-Net\n",
            "import network module: networks.lora\n",
            "create LoRA network. base dim (rank): 32, alpha: 16\n",
            "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
            "create LoRA for Text Encoder 1:\n",
            "create LoRA for Text Encoder 2:\n",
            "create LoRA for Text Encoder: 264 modules.\n",
            "create LoRA for U-Net: 722 modules.\n",
            "enable LoRA for text encoder\n",
            "enable LoRA for U-Net\n",
            "prepare optimizer, data loader etc.\n",
            "learning rate is too low. If using D-Adaptation or Prodigy, set learning rate around 1.0 / 学習率が低すぎるようです。D-AdaptationまたはProdigyの使用時は1.0前後の値を指定してください: lr=0.0001\n",
            "recommend option: lr=1.0 / 推奨は1.0です\n",
            "use Prodigy optimizer | {'decouple': True, 'weight_decay': 0.01, 'betas': (0.9, 0.999), 'd_coef': 2, 'use_bias_correction': True}\n",
            "Using decoupled weight decay\n",
            "override steps. steps for 10 epochs is / 指定エポックまでのステップ数: 1920\n",
            "running training / 学習開始\n",
            "  num train images * repeats / 学習画像の数×繰り返し回数: 384\n",
            "  num reg images / 正則化画像の数: 0\n",
            "  num batches per epoch / 1epochのバッチ数: 384\n",
            "  num epochs / epoch数: 10\n",
            "  batch size per device / バッチサイズ: 1\n",
            "  gradient accumulation steps / 勾配を合計するステップ数 = 2\n",
            "  total optimization steps / 学習ステップ数: 1920\n",
            "steps:   0% 0/1920 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkalthommusa12345\u001b[0m (\u001b[33mkalthommusa12345-umm-al-qura-university-j-mi-ah-umm-al-qur-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.20.1 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LoRA/logs/20250627214903/wandb/run-20250627_215138-xuai9jvd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mexalted-snow-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kalthommusa12345-umm-al-qura-university-j-mi-ah-umm-al-qur-/Saudi-Heritage-Lora%20\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kalthommusa12345-umm-al-qura-university-j-mi-ah-umm-al-qur-/Saudi-Heritage-Lora%20/runs/xuai9jvd\u001b[0m\n",
            "\n",
            "epoch 1/10\n",
            "steps:  10% 192/1920 [15:28<2:19:17,  4.84s/it, loss=0.0766]\n",
            "epoch 2/10\n",
            "steps:  20% 384/1920 [30:54<2:03:36,  4.83s/it, loss=0.0781]\n",
            "epoch 3/10\n",
            "steps:  30% 576/1920 [46:19<1:48:06,  4.83s/it, loss=0.0775]\n",
            "epoch 4/10\n",
            "steps:  40% 768/1920 [1:01:43<1:32:34,  4.82s/it, loss=0.0793]\n",
            "epoch 5/10\n",
            "steps:  50% 960/1920 [1:17:05<1:17:05,  4.82s/it, loss=0.0704]\n",
            "epoch 6/10\n",
            "steps:  60% 1152/1920 [1:32:27<1:01:38,  4.82s/it, loss=0.0714]\n",
            "epoch 7/10\n",
            "steps:  70% 1344/1920 [1:47:52<46:13,  4.82s/it, loss=0.0749]\n",
            "epoch 8/10\n",
            "steps:  80% 1536/1920 [2:03:20<30:50,  4.82s/it, loss=0.0746]\n",
            "epoch 9/10\n",
            "steps:  90% 1728/1920 [2:18:42<15:24,  4.82s/it, loss=0.074] \n",
            "epoch 10/10\n",
            "steps: 100% 1920/1920 [2:34:05<00:00,  4.82s/it, loss=0.0775]\n",
            "generating sample images at step / サンプル画像生成 ステップ: 1920\n",
            "prompt: woman wearing najdi dress, ornate head accessory, traditional saudi fashion, heritage style\n",
            "negative_prompt: 3d render, smooth, plastic, blurry, grainy, low-resolution, deep-fried, oversaturated\n",
            "height: 768\n",
            "width: 768\n",
            "sample_steps: 28\n",
            "scale: 12\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:21,  1.25it/s]\u001b[A\n",
            "  7% 2/28 [00:01<00:18,  1.38it/s]\u001b[A\n",
            " 11% 3/28 [00:02<00:17,  1.44it/s]\u001b[A\n",
            " 14% 4/28 [00:02<00:16,  1.48it/s]\u001b[A\n",
            " 18% 5/28 [00:03<00:15,  1.50it/s]\u001b[A\n",
            " 21% 6/28 [00:04<00:14,  1.51it/s]\u001b[A\n",
            " 25% 7/28 [00:04<00:13,  1.52it/s]\u001b[A\n",
            " 29% 8/28 [00:05<00:13,  1.53it/s]\u001b[A\n",
            " 32% 9/28 [00:06<00:12,  1.53it/s]\u001b[A\n",
            " 36% 10/28 [00:06<00:11,  1.53it/s]\u001b[A\n",
            " 39% 11/28 [00:07<00:11,  1.54it/s]\u001b[A\n",
            " 43% 12/28 [00:07<00:10,  1.53it/s]\u001b[A\n",
            " 46% 13/28 [00:08<00:09,  1.53it/s]\u001b[A\n",
            " 50% 14/28 [00:09<00:09,  1.54it/s]\u001b[A\n",
            " 54% 15/28 [00:09<00:08,  1.53it/s]\u001b[A\n",
            " 57% 16/28 [00:10<00:07,  1.53it/s]\u001b[A\n",
            " 61% 17/28 [00:11<00:07,  1.54it/s]\u001b[A\n",
            " 64% 18/28 [00:11<00:06,  1.53it/s]\u001b[A\n",
            " 68% 19/28 [00:12<00:05,  1.53it/s]\u001b[A\n",
            " 71% 20/28 [00:13<00:05,  1.53it/s]\u001b[A\n",
            " 75% 21/28 [00:13<00:04,  1.54it/s]\u001b[A\n",
            " 79% 22/28 [00:14<00:03,  1.53it/s]\u001b[A\n",
            " 82% 23/28 [00:15<00:03,  1.53it/s]\u001b[A\n",
            " 86% 24/28 [00:15<00:02,  1.54it/s]\u001b[A\n",
            " 89% 25/28 [00:16<00:01,  1.54it/s]\u001b[A\n",
            " 93% 26/28 [00:17<00:01,  1.54it/s]\u001b[A\n",
            " 96% 27/28 [00:17<00:00,  1.54it/s]\u001b[A\n",
            "100% 28/28 [00:18<00:00,  1.54it/s]\u001b[A\n",
            "                                   \u001b[Aprompt: najdi man, traditional embroidered thobe, shemagh headscarf, saudi central region\n",
            "negative_prompt: 3d render, smooth, plastic, blurry, grainy, low-resolution, deep-fried, oversaturated\n",
            "height: 768\n",
            "width: 768\n",
            "sample_steps: 28\n",
            "scale: 12\n",
            "\n",
            "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 1/28 [00:00<00:17,  1.56it/s]\u001b[A\n",
            "  7% 2/28 [00:01<00:17,  1.51it/s]\u001b[A\n",
            " 11% 3/28 [00:01<00:16,  1.52it/s]\u001b[A\n",
            " 14% 4/28 [00:02<00:15,  1.53it/s]\u001b[A\n",
            " 18% 5/28 [00:03<00:15,  1.52it/s]\u001b[A\n",
            " 21% 6/28 [00:03<00:14,  1.53it/s]\u001b[A\n",
            " 25% 7/28 [00:04<00:13,  1.53it/s]\u001b[A\n",
            " 29% 8/28 [00:05<00:13,  1.53it/s]\u001b[A\n",
            " 32% 9/28 [00:05<00:12,  1.53it/s]\u001b[A\n",
            " 36% 10/28 [00:06<00:11,  1.53it/s]\u001b[A\n",
            " 39% 11/28 [00:07<00:11,  1.53it/s]\u001b[A\n",
            " 43% 12/28 [00:07<00:10,  1.53it/s]\u001b[A\n",
            " 46% 13/28 [00:08<00:09,  1.53it/s]\u001b[A\n",
            " 50% 14/28 [00:09<00:09,  1.53it/s]\u001b[A\n",
            " 54% 15/28 [00:09<00:08,  1.53it/s]\u001b[A\n",
            " 57% 16/28 [00:10<00:07,  1.53it/s]\u001b[A\n",
            " 61% 17/28 [00:11<00:07,  1.53it/s]\u001b[A\n",
            " 64% 18/28 [00:11<00:06,  1.53it/s]\u001b[A\n",
            " 68% 19/28 [00:12<00:05,  1.53it/s]\u001b[A\n",
            " 71% 20/28 [00:13<00:05,  1.53it/s]\u001b[A\n",
            " 75% 21/28 [00:13<00:04,  1.53it/s]\u001b[A\n",
            " 79% 22/28 [00:14<00:03,  1.53it/s]\u001b[A\n",
            " 82% 23/28 [00:15<00:03,  1.53it/s]\u001b[A\n",
            " 86% 24/28 [00:15<00:02,  1.52it/s]\u001b[A\n",
            " 89% 25/28 [00:16<00:01,  1.53it/s]\u001b[A\n",
            " 93% 26/28 [00:17<00:01,  1.52it/s]\u001b[A\n",
            " 96% 27/28 [00:17<00:00,  1.53it/s]\u001b[A\n",
            "100% 28/28 [00:18<00:00,  1.53it/s]\u001b[A\n",
            "                                   \u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss/average ▆▆█▆▆▆▅▅▅▅▇▅▅▅▃▇▆▅▄▂▄▅▅▃▁▂▃▄▆▅▅▄▅▅▄▅▄▄▆▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss/current ▁▂▃▄▁▂▂▂▃▂▁▁▃▃▁▂▂▂▃▁▁▁▁▂▃▁▁▁▃▂▁▃█▂▁▁▁▁▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        lr/d*lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: lr/textencoder ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        lr/unet ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss/average 0.07753\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss/current 0.29648\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        lr/d*lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: lr/textencoder 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        lr/unet 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mexalted-snow-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/kalthommusa12345-umm-al-qura-university-j-mi-ah-umm-al-qur-/Saudi-Heritage-Lora%20/runs/xuai9jvd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/kalthommusa12345-umm-al-qura-university-j-mi-ah-umm-al-qur-/Saudi-Heritage-Lora%20/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjY2MjMzNDQwNA==/version_details/v1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m/content/LoRA/logs/20250627214903/wandb/run-20250627_215138-xuai9jvd/logs\u001b[0m\n",
            "\n",
            "saving checkpoint: /content/drive/MyDrive/Saudi-Heritage-GenAI/saudi-heritage-lora-wb/Saudi-Heritage-Lora .safetensors\n",
            "model saved.\n",
            "steps: 100% 1920/1920 [2:34:57<00:00,  4.84s/it, loss=0.0775]\n"
          ]
        }
      ],
      "source": [
        "#@title ## **4.5. Start Training**\n",
        "import os\n",
        "import toml\n",
        "\n",
        "#@markdown Check your config here if you want to edit something:\n",
        "#@markdown - `sample_prompt` : /content/LoRA/config/sample_prompt.toml\n",
        "#@markdown - `config_file` : /content/LoRA/config/config_file.toml\n",
        "\n",
        "\n",
        "#@markdown You can import config from another session if you want.\n",
        "\n",
        "sample_prompt   = \"/content/LoRA/config/sample_prompt.toml\" #@param {type:'string'}\n",
        "config_file     = \"/content/LoRA/config/config_file.toml\" #@param {type:'string'}\n",
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        contents = f.read()\n",
        "    return contents\n",
        "\n",
        "def train(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "\n",
        "    return args\n",
        "\n",
        "accelerate_conf = {\n",
        "    \"config_file\" : \"/content/kohya-trainer/accelerate_config/config.yaml\",\n",
        "    \"num_cpu_threads_per_process\" : 1,\n",
        "}\n",
        "\n",
        "train_conf = {\n",
        "    \"sample_prompts\"  : sample_prompt if os.path.exists(sample_prompt) else None,\n",
        "    \"config_file\"     : config_file,\n",
        "    \"wandb_api_key\"   : wandb_api_key if wandb_api_key else None\n",
        "}\n",
        "\n",
        "accelerate_args = train(accelerate_conf)\n",
        "train_args = train(train_conf)\n",
        "\n",
        "final_args = f\"accelerate launch {accelerate_args} sdxl_train_network.py {train_args}\"\n",
        "\n",
        "os.chdir(repo_dir)\n",
        "!{final_args}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_57sgHbCvirn"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(300)\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}